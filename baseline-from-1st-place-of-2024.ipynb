{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91496,"databundleVersionId":11483707,"sourceType":"competition"},{"sourceId":9515958,"sourceType":"datasetVersion","datasetId":5793177},{"sourceId":158171,"sourceType":"modelInstanceVersion","modelInstanceId":134422,"modelId":157175}],"dockerImageVersionId":30762,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":1059.350941,"end_time":"2025-04-04T14:08:54.10074","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-04-04T13:51:14.749799","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# This Notebook is forked from the 1st Place Winner of ARC Prize 2024\n\n## Dark AGI's ARC-2025 Open Source Commitment: All submissions in this competition will be open-sourced to help the community explore AGI capable of achieving 85%+ scores.\n\n**Competition Link:** [ARC Prize 2024](https://www.kaggle.com/competitions/arc-prize-2024)  \n**Original Notebook:** [arc-prize-2024-solution-by-the-architects](https://www.kaggle.com/code/dfranzen/arc-prize-2024-solution-by-the-architects?scriptVersionId=211637468)\n\n---\n\n## Modifications Implemented\n\n1. **4-GPU Support**\n   - Extended multi-GPU implementation from 2 to 4 GPUs\n   - Modified dataset splitting logic in `prepare_dataset` to evenly distribute work\n   - Added training and inference processes for GPU 2 and GPU 3\n   - Updated subprocess monitoring to wait for all 8 processes (4 training + 4 inference)\n   - Improved resource utilization and inference throughput by 2x\n\n2. **Enhanced Reproducibility**\n   - Added global seed control (GLOBAL_SEED = 42) with per-GPU deterministic seeding\n   - Applied consistent seed values to all randomized operations for reproducible results\n   - Disabled non-deterministic algorithms to ensure consistent outputs across runs\n   - Implemented seed-based task distribution for consistent GPU workloads\n\n3. **Comprehensive Visualization**\n   - Implemented data visualization for both training and inference phases:\n     - Color-coded grid displays for ARC tasks with intuitive color mapping\n     - Side-by-side comparisons of inputs, ground truth, and model predictions\n   - Added multi-GPU result comparison showing prediction quality across all GPUs\n   - Created task-specific visualizations showing training examples, test inputs, and prediction attempts\n   - Calculated detailed accuracy metrics with statistical breakdowns:\n     - Per-attempt success rates for first and second predictions\n     - Overall accuracy percentages for both individual attempts\n     - Combined success rate for either prediction attempt\n     - Shape and value distribution analysis for predictions vs ground truth\n     - Non-zero prediction completion rate and zero-prediction filtering","metadata":{"papermill":{"duration":0.0077,"end_time":"2025-04-04T13:51:17.531445","exception":false,"start_time":"2025-04-04T13:51:17.523745","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Copyright 2024 Daniel Franzen and Jan Disselhoff\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.","metadata":{"execution":{"iopub.execute_input":"2025-04-04T13:51:17.54615Z","iopub.status.busy":"2025-04-04T13:51:17.545902Z","iopub.status.idle":"2025-04-04T13:51:17.549486Z","shell.execute_reply":"2025-04-04T13:51:17.548951Z"},"papermill":{"duration":0.012497,"end_time":"2025-04-04T13:51:17.550971","exception":false,"start_time":"2025-04-04T13:51:17.538474","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# This notebook contains our winning submission to the ARC Prize 2024 Kaggle competition,\n# scoring 53.5 points on the private evaluation set.\n# the ARChitects (Daniel Franzen and Jan Disselhoff)","metadata":{"execution":{"iopub.execute_input":"2025-04-04T13:51:17.565451Z","iopub.status.busy":"2025-04-04T13:51:17.565051Z","iopub.status.idle":"2025-04-04T13:51:17.567719Z","shell.execute_reply":"2025-04-04T13:51:17.567193Z"},"papermill":{"duration":0.011385,"end_time":"2025-04-04T13:51:17.56911","exception":false,"start_time":"2025-04-04T13:51:17.557725","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Runner Module Explanation\n\nThis Python module (`model_runner.py`) is a comprehensive toolkit for working with language models, focusing on efficiency and performance optimization. The code is designed to handle various aspects of model management including loading, training, inference, and optimization.\n\n## Key Components\n\n### Tokenizer Optimization\nThe module provides several functions for optimizing tokenizers:\n- `indices_required_for_merges` identifies necessary token IDs for BPE merges\n- `remove_unused_merges` cleans up unused merge rules\n- `shrink_tokenizer_vocab` reduces vocabulary size while maintaining model functionality\n- `remove_tokenizer_normalizer` removes normalizer components when not needed\n\n### Model Size Reduction\nThe code includes specialized functionality for reducing model size:\n- `shrink_model_embeddings` resizes embedding tables to match reduced vocabularies\n- `shrink_embeddings` orchestrates the entire embedding reduction process\n- Support for 4-bit quantization through `transformers_4bit` and `unsloth_4bit` loading modes\n\n### Model Management\nSeveral utilities handle model loading and manipulation:\n- `prepare_model` provides a unified interface for model loading with various options\n- `merge_peft_into_base` merges fine-tuned adapters into base models\n- `fix_dtypes` ensures consistent data types across model components\n- `save_model` handles model saving with options for merging adapters\n\n### Training Functions\nThe module supports efficient fine-tuning:\n- `training_run` handles the training process with support for both standard and optimized trainers\n- The `Retrainer` class enables retraining with data augmentation\n- Support for gradient accumulation fixes and packing optimizations\n\n### Inference\nComprehensive inference capabilities:\n- `inference_run_v2` orchestrates inference runs across datasets\n- `inference_turbo_dfs` implements a depth-first search approach for higher quality outputs\n- `inference_step` handles token generation with various decoding strategies\n\n### Result Processing\nThe `Decoder` class provides extensive functionality:\n- Tracks and evaluates generated outputs against reference solutions\n- Calculates accuracy metrics based on exact matches\n- Supports probability tracking for solution ranking\n- Benchmarks different selection algorithms\n\n### Utilities\nHelpful utilities include:\n- Compressed storage for inference results via `inference_save`/`inference_load` \n- PEFT weight management functions\n- GPU memory tracking via `mem_info`\n\nThis module appears to be built for competitive or research applications where optimizing model efficiency and generation quality is critical.","metadata":{"papermill":{"duration":0.00656,"end_time":"2025-04-04T13:51:17.582367","exception":false,"start_time":"2025-04-04T13:51:17.575807","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%%writefile model_runner.py\nimport json\nimport os, sys\nimport bz2\nimport pickle\nimport numpy as np\nfrom tqdm import tqdm\n\ndef indices_required_for_merges(keep_indices, vocab, merges):\n    merges_lookup = {}\n    for m in merges:\n        a, b = m.split(' ') if isinstance(m, str) else m\n        key = vocab[f'{a}{b}']\n        if key not in merges_lookup: merges_lookup[key] = set()\n        merges_lookup[key].add(vocab[a])\n        merges_lookup[key].add(vocab[b])\n    to_process = list(keep_indices)\n    while len(to_process):\n        for w in merges_lookup.get(to_process.pop(), []):\n            if w not in keep_indices:\n                keep_indices[w] = None\n                to_process.append(w)\n    return keep_indices\n\ndef remove_unused_merges(merges, vocab):\n    return [f'{a} {b}' for a, b in [m.split(' ') if isinstance(m, str) else m for m in merges] if all(w in vocab for w in [a, b, a + b])]\n\ndef map_special_tokens(data, mapping=None):\n    tokens = set()\n    if isinstance(data, dict):\n        special = data.get('special_tokens')\n        if special is not None:\n            for v in special.values():\n                tokens.update(v['ids'])\n                if mapping is not None:\n                    v['ids'] = [mapping.get(i) for i in v['ids'] if i in mapping]\n    for v in (data.values() if isinstance(data, dict) else data if isinstance(data, list) else []):\n        tokens.update(map_special_tokens(v, mapping))\n    return tokens\n\ndef remove_tokenizer_normalizer(tokenizer):\n    from tokenizers import Tokenizer\n    assert tokenizer.is_fast\n    tokenizer_json = json.loads(tokenizer._tokenizer.to_str())\n    if tokenizer_json.get('normalizer') is not None:\n        tokenizer_json['normalizer'] = None\n        tokenizer._tokenizer = Tokenizer.from_str(json.dumps(tokenizer_json))\n\ndef shrink_tokenizer_vocab(tokenizer, keep_indices, keep_special_tokens, keep_token_order):\n    from tokenizers import Tokenizer\n    assert tokenizer.is_fast\n    tokenizer_json = json.loads(tokenizer._tokenizer.to_str())\n    assert tokenizer_json['model']['type'] == \"BPE\"\n    if keep_special_tokens:\n        keep_indices.update({k: None for k in tokenizer.all_special_ids})\n        keep_indices.update({k: None for k in map_special_tokens(tokenizer_json.get('post_processor'))})\n    keep_indices = indices_required_for_merges(keep_indices, tokenizer_json['model']['vocab'], tokenizer_json['model']['merges'])\n    if keep_token_order: keep_indices = sorted(keep_indices)\n    mapping = {old: new for new, old in enumerate(keep_indices)}\n    tokenizer_json['model']['vocab'] = {k: mapping[v] for k, v in tokenizer_json['model']['vocab'].items() if v in mapping}\n    tokenizer_json['model']['merges'] = remove_unused_merges(tokenizer_json['model']['merges'], tokenizer_json['model']['vocab'])\n    special_tokens_order = [t['id'] for t in tokenizer_json['added_tokens']]\n    assert special_tokens_order==sorted(special_tokens_order)\n    tokenizer_json['added_tokens'] = sorted([{**t, 'id': mapping[t['id']]} for t in tokenizer_json['added_tokens'] if t['id'] in mapping], key=lambda t: t['id'])\n    map_special_tokens(tokenizer_json.get('post_processor'), mapping)\n    tokenizer._tokenizer = Tokenizer.from_str(json.dumps(tokenizer_json))\n    return mapping, keep_indices\n\ndef shrink_model_embeddings(model, keep_indices, mapping):\n    import torch\n    with torch.no_grad():\n        row_select = torch.tensor(list(keep_indices))\n        new_embed_t = torch.index_select(model.get_input_embeddings().weight.data, 0, row_select.to(model.get_input_embeddings().weight.data.device))\n        new_lm_head = torch.index_select(model.get_output_embeddings().weight.data, 0, row_select.to(model.get_output_embeddings().weight.data.device))\n        model.resize_token_embeddings(len(keep_indices))\n        model.get_input_embeddings().weight.data[:] = new_embed_t\n        model.get_output_embeddings().weight.data[:] = new_lm_head\n        for config in [model.config, model.generation_config]:\n            for k, v in list(config.to_dict().items()):\n                if k.endswith('token_id'):\n                    setattr(config, k, [mapping.get(t) for t in v] if isinstance(v, list) else mapping.get(v))\n\ndef shrink_embeddings(model, tokenizer, corpus=None, keep_token_ids=[], keep_tokens=[], remove_token_ids=[], keep_model_tokens=True, keep_special_tokens=True, keep_normalizer=False, keep_token_order=True):\n    if not keep_normalizer: remove_tokenizer_normalizer(tokenizer)\n    from collections import OrderedDict  # use as OrderedSet\n    keep_indices = OrderedDict()\n    keep_indices.update({k: None for k in keep_token_ids})\n    keep_indices.update({tokenizer.vocab[t]: None for t in keep_tokens})\n    if corpus is not None: keep_indices.update({k: None for k in tokenizer(corpus)['input_ids']})\n    if keep_model_tokens:\n        for config in [model.config, model.generation_config]:\n            for k, v in config.to_dict().items():\n                if k.endswith('token_id'):\n                    keep_indices.update({k: None for k in (v if isinstance(v, list) else [v])})\n    keep_indices.pop(None, None)\n    for idx in remove_token_ids: keep_indices.pop(idx, None)\n    mapping, keep_indices = shrink_tokenizer_vocab(tokenizer, keep_indices, keep_special_tokens, keep_token_order)\n    shrink_model_embeddings(model, keep_indices, mapping=mapping)\n    return mapping\n\ndef fix_dtypes(model, fix_weights=True, fix_quant_states=True):\n    import torch\n    for module in model.modules():\n        weight = getattr(module, 'weight', None)\n        if weight is not None:\n            if torch.is_floating_point(weight):\n                if fix_weights and weight.dtype!=model.dtype:\n                    module.to(model.dtype)\n            else:\n                qs = getattr(weight, 'quant_state', None)\n                if qs is not None:\n                    if fix_quant_states and qs.dtype!=model.dtype:\n                        qs.dtype = model.dtype\n    return model\n\ndef merge_peft_into_base(model):\n    print('*** Merge peft model into base model...')\n    assert is_peft_model(model)\n    return fix_dtypes(model.merge_and_unload())\n\ndef save_model(store_path, model=None, tokenizer=None, merge=False):\n    if merge: model = merge_peft_into_base(model)\n    if store_path is not None:\n        assert model is not None or tokenizer is not None\n        print(f\"*** Saving{' merged' if merge else ''} model/tokenizer to '{store_path}'...\")\n        if model is not None: model.save_pretrained(store_path)\n        if tokenizer is not None:\n            tokenizer.save_pretrained(store_path)\n            to_delete = os.path.join(store_path, 'tokenizer.model')\n            if os.path.isfile(to_delete): os.remove(to_delete)\n    return model\n\ndef is_unsloth_model(model):\n    return model.model_tags is not None and 'unsloth' in model.model_tags\n\ndef is_peft_model(model):\n    return hasattr(model, 'peft_type')\n\ndef download_model(repo_id, store_path, get_name=lambda n: os.path.join(n.replace('/', '--'), 'transformers', 'default', '1')):\n    import os\n    if os.path.exists(repo_id): return repo_id\n    model_path = os.path.join(store_path, get_name(repo_id))\n    if not os.path.exists(model_path):\n        from huggingface_hub import snapshot_download\n        download_path = snapshot_download(repo_id=repo_id)\n        os.makedirs(os.path.split(model_path)[0], exist_ok=True)\n        os.symlink(download_path, model_path, target_is_directory=True)\n    return model_path\n\ndef get_and_fix_peft_weights(store):\n    print(f\"*** Load peft state_dict from '{store}'...\")\n    from peft import load_peft_weights\n    state_dict = load_peft_weights(store)\n    for k in list(state_dict.keys()):\n        if 'modules_to_save' in k:\n            del state_dict[k]\n            original_module_key = k.replace('.modules_to_save.', '.original_module.')\n            if original_module_key in state_dict: del state_dict[original_module_key]\n            assert k.replace('.modules_to_save.', '.') in state_dict\n    return state_dict\n\ndef set_peft_weights(model, state_dict):\n    print(f\"*** Set model state_dict...\")\n    from peft import set_peft_model_state_dict\n    res = set_peft_model_state_dict(model, state_dict)\n    assert not res.unexpected_keys\n\ndef load_peft_state(model, store):\n    set_peft_weights(model, get_and_fix_peft_weights(store))\n\ndef prepare_model(model, mode, tokenizer=None, formatter=None, shrink_embedding=False, dequantize=False, peft=[], local_files_only=False, add_special_tokens={}, set_pad_token=None, keep_tokens=[], keep_normalizer=None, peft_trainable=True, device_map=None, tf_grad_cp=True, tf_use_fa2=True, **kwargs):\n    if isinstance(model, str):\n        assert tokenizer is None\n        print(f\"*** Load base model and tokenizer from '{model}'...\")\n        if mode=='unsloth_4bit':\n            assert device_map is None, 'unsupported'\n            from unsloth import FastLanguageModel\n            model, tokenizer = FastLanguageModel.from_pretrained(model_name=model, dtype=None, load_in_4bit=True, local_files_only=local_files_only, **kwargs)\n        elif mode in ['transformers', 'transformers_bf16', 'transformers_4bit', 'transformers_bf16_4bit', 'tokenizer_only']:\n            import torch\n            model_load_args = {}\n            if device_map is not None: model_load_args['device_map'] = device_map\n            if tf_use_fa2: model_load_args['attn_implementation'] = 'flash_attention_2'\n            if mode in ['transformers_bf16', 'transformers_bf16_4bit']: model_load_args['torch_dtype'] = torch.bfloat16\n            elif mode in ['transformers_4bit', 'transformers_bf16_4bit']:\n                from transformers import BitsAndBytesConfig\n                nf4_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type='nf4', bnb_4bit_use_double_quant=True, bnb_4bit_compute_dtype=torch.bfloat16)\n                model_load_args['quantization_config'] = nf4_config\n            from transformers import AutoTokenizer, AutoModelForCausalLM\n            tokenizer = AutoTokenizer.from_pretrained(model, local_files_only=local_files_only, **kwargs)\n            model = AutoModelForCausalLM.from_pretrained(model, **model_load_args) if mode!='tokenizer_only' else None\n            if tf_grad_cp and model is not None: model.gradient_checkpointing_enable()\n        else: raise NotImplementedError('Unknown mode.')\n    if add_special_tokens: tokenizer.add_special_tokens(add_special_tokens)\n    if set_pad_token is not None: tokenizer.pad_token = set_pad_token\n    if formatter is not None and not hasattr(formatter, 'corpus'):\n        formatter = formatter(tokenizer=tokenizer)\n    if (shrink_embedding<len(tokenizer.vocab) if type(shrink_embedding)==int else shrink_embedding) or keep_normalizer is False:\n        print('*** Shrink embedding...')\n        embedding_size_before_shrink = len(tokenizer.vocab)\n        mapping = shrink_embeddings(model, tokenizer, formatter.get_corpus(), keep_tokens=keep_tokens, keep_normalizer=keep_normalizer)\n        print(f'*** -> Reduced embedding size from {embedding_size_before_shrink} to {len(mapping)} words.')\n    if dequantize:\n        print(f'*** Dequantize model...')\n        model = model.dequantize()\n    if len(peft):\n        peft_trained = True if is_peft_model(model) else None\n        for i, m in enumerate(peft):\n            if peft_trained is True: model, peft_trained = merge_peft_into_base(model), None\n            if isinstance(m, str):\n                if peft_trained is False:\n                    _, peft_trained = load_peft_state(model, m), True\n                else:\n                    print(f\"*** Load peft model from '{m}'...\")\n                    # be careful when using unsloth - using PeftModel to load the model will not apply unsloth optimizations\n                    from peft import PeftModel\n                    model, peft_trained = PeftModel.from_pretrained(model, m, trainable=peft_trainable), True\n            else:\n                assert peft_trained is None\n                if isinstance(m, dict):\n                    print('*** Create new peft model...')\n                    if is_unsloth_model(model):\n                        from unsloth import FastLanguageModel\n                        my_get_peft_model = FastLanguageModel.get_peft_model\n                    else:\n                        from peft import LoraConfig, get_peft_model\n                        my_get_peft_model = lambda model, **kwargs: get_peft_model(model, LoraConfig(**kwargs))\n                    model, peft_trained = my_get_peft_model(model, **m), False\n                else: assert m is None\n    return model, tokenizer, formatter\n\ndef training_run(model, formatter, dataset, train_args, max_seq_length, merge=False, store=None, packing=False, grad_acc_fix=False, optimizers=None):\n    assert merge is False, \"merge after training does not seen to work (at least with unsloth, saved merged model will cointain the untrained weights!)\"\n    import torch\n    from datasets import Dataset\n    add_train_args = {}\n    if is_unsloth_model(model):\n        from unsloth import FastLanguageModel\n        from unsloth import UnslothTrainer as Trainer\n        from unsloth import UnslothTrainingArguments as TrainingArguments\n        from unsloth import is_bfloat16_supported\n        FastLanguageModel.for_training(model)\n        add_train_args.update(fp16=not is_bfloat16_supported(), bf16=is_bfloat16_supported())\n    else:\n        from trl import SFTConfig as TrainingArguments\n        from trl import SFTTrainer as Trainer\n        model.train()\n        add_train_args.update(bf16=True)\n\n    formatter.tokenizer.padding_side = 'right'\n    if is_unsloth_model(model):\n        for convert_to_float in [model.get_input_embeddings(), model.get_output_embeddings()]:\n            if convert_to_float.weight.dtype!=torch.float32: convert_to_float.to(torch.float32)\n\n    add_args = {}\n    if optimizers is not None: add_args['optimizers'] = optimizers\n\n    trainer = Trainer(\n        model=model,\n        tokenizer=formatter.tokenizer,\n        data_collator=formatter.get_data_collator(),\n        train_dataset=Dataset.from_list(dataset.as_list(formatter)),\n        dataset_text_field=\"text\",\n        max_seq_length=max_seq_length,\n        dataset_num_proc=None,\n        packing=packing,  # Can make training 5x faster for short sequences.\n        **add_args,\n        args=TrainingArguments(\n            **add_train_args,\n            **train_args\n        ),\n    )\n\n    print('*** Start training run...')\n    if grad_acc_fix and is_unsloth_model(model):\n        from unsloth import unsloth_train\n        trainer_stats = unsloth_train(trainer)\n    else:\n        if is_unsloth_model(model) and train_args['gradient_accumulation_steps']>1: print('*** WARNING: using faulty unsloth gradient accumulation')\n        trainer_stats = trainer.train()\n    try: print(f'*** -> Training took {trainer_stats.metrics[\"train_runtime\"]} seconds.')\n    except: pass\n    if store is not None: save_model(store, model, formatter.tokenizer, merge=merge)\n    return model, trainer_stats\n\ndef inference_load(store, keys=True, result_dict=None, always_read_from_file=False):\n    if result_dict is None: result_dict = {}\n    if store is not None:\n        if keys is True: keys = os.listdir(store)\n        for key in keys:\n            if always_read_from_file or key not in result_dict:\n                try:\n                    with bz2.BZ2File(os.path.join(store, key)) as f: result_dict[key] = pickle.load(f)\n                except: continue\n    return result_dict\n\ndef inference_save(store, key, outputs):\n    if store is not None:\n        os.makedirs(store, exist_ok=True)\n        with bz2.BZ2File(os.path.join(store, key), 'w') as f: pickle.dump(outputs, f)\n\nclass Decoder(object):\n    def __init__(self, formatter, dataset, n_guesses, max_outputs=None, frac_score=False, quiet=False, name='', additional_decoders=None, prob_baseline=None):\n        self.formatter = formatter\n        self.dataset = dataset\n        self.n_guesses = n_guesses\n        self.decoded_results = {}\n        self.correct_solutions = {}\n        self.keys_lim = set()\n        self.keys_all = set()\n        self.mult_cnt = {}\n        self.keys_cnt = {}\n        self.frac_score = frac_score\n        self.max_outputs = max_outputs\n        self.quiet = quiet\n        self.input_len = [{} if formatter is not None and formatter.tokenizer is None else ds.get_lengths(formatter, name='input') for ds in [dataset, dataset.mod(np.transpose, keep_key=True)]]\n        self.reply_len = [{} if formatter is not None and formatter.tokenizer is None else ds.get_lengths(formatter, name='reply') for ds in [dataset, dataset.mod(np.transpose, keep_key=True)]]\n        self.additional_decoders = additional_decoders\n        self.name = name\n        self.prob_tracker = {}\n        self.prob_tracker_best = {}\n        self.prob_baseline = prob_baseline\n\n    def score(self, *to_score):\n        scores = [(sum(1/self.mult_cnt[k.split('_')[0]] for k in s) if self.frac_score else len(s)) for s in to_score]\n        score_cnt = len(self.mult_cnt if self.frac_score else self.keys_cnt)\n        return scores, score_cnt\n\n    def from_store(self, store, **kwargs):\n        for key, outputs in inference_load(store).items():\n            self.process(key, outputs, **kwargs)\n        return self\n\n    def score_fmt(self, v):\n        return f'{v:5.1f}' if self.frac_score else f'{v:3}'\n\n    def process_single_output(self, key, output_len, decoded, print_func=print, len_info=None, device_info=None):\n        import numpy as np\n        inv_mod = {k: v if k.endswith('val') else self.dataset.invert_mod(v, key, inv_perm=(k.startswith('output') or k.startswith('score_all'))) for k, v in decoded.items()}\n        base_key = key.split('.')[0]\n        self.decoded_results[base_key] = self.decoded_results.get(base_key, {})\n        self.decoded_results[base_key][key] = inv_mod\n        output = inv_mod.get('output')\n        score = inv_mod.get('score')\n\n        # quick scoring\n        self.keys_cnt[base_key] = self.keys_cnt.get(base_key, 0) + 1\n        mult_key, mult_sub = (base_key.split('_') + ['0'])[:2]\n        self.mult_cnt[mult_key] = max(self.mult_cnt.get(mult_key, 0), int(mult_sub) + 1)\n        if len(self.dataset.replies):\n            correct_solution = self.dataset.replies.get(base_key)\n            if correct_solution is not None:\n                correct_solution = correct_solution[0]\n                self.correct_solutions[base_key] = correct_solution\n                is_correct = correct_solution is not None and np.array_equal(correct_solution, output)\n                if is_correct:\n                    self.keys_all.add(base_key)\n                    if self.keys_cnt[base_key] <= self.n_guesses: self.keys_lim.add(base_key)\n            corr_str = 'cant_decode' if output is None else 'sol_unknown' if correct_solution is None else 'ALL_CORRECT' if is_correct else 'bad_xy_size' if np.shape(correct_solution)!=np.shape(output) else 'bad_content'\n            (score_lim, score_all), score_cnt = self.score(self.keys_lim, self.keys_all)\n\n            tp_arr = (key.count('transpose') + key.count('rot90')) % 2\n            msc = None if score is None else np.sum(score)\n            fsc = inv_mod.get('score_val')\n            if output is not None and fsc is not None:\n                pt = self.prob_tracker[base_key] = self.prob_tracker.get(base_key, {})\n                hash = tuple(map(tuple, output))\n                prob = pt[hash] = pt.get(hash, 0) + (np.exp(fsc) if self.prob_baseline is None else fsc - np.log(self.prob_baseline))\n                current_best = self.prob_tracker_best.get(base_key)\n                if current_best is None or current_best[0]<prob:\n                    self.prob_tracker_best[base_key] = (prob, output)\n            fmt_name = f'{self.name}: ' if self.name else ''\n            msc_print = f'{min(-msc, 9.99999):7.5f}' if msc is not None else 'unknown'\n            fsc_print = f'{min(-fsc, 9.99999):7.5f}' if fsc is not None else 'unknown'\n            if not self.quiet: print_func(f\" {fmt_name}acc: {self.score_fmt(score_lim)}/{score_cnt:3}={min(score_lim/score_cnt, 0.999):5.1%} (2-guess), {self.score_fmt(score_all)}/{score_cnt:3}={min(score_all/score_cnt, 0.999):5.1%} (any);{f' {device_info}' if device_info else ''} tok:{self.input_len[tp_arr].get(base_key, '?'):>4}+{self.reply_len[tp_arr].get(base_key, '?'):>3}>{'n/a' if output_len is None else output_len:>3} {corr_str}:{msc_print}|{fsc_print} [{key}]\")\n\n    def get_current_best(self, base_key):\n        current_best = self.prob_tracker_best.get(base_key)\n        return None if current_best is None else current_best[1]\n\n    def process_single_decode(self, key, de_tokenized, print_func=print, **kwargs):\n        if len(de_tokenized)==3 and not isinstance(de_tokenized[1], float):  # for backwards compatibility\n            output_len, *data = de_tokenized\n            score_val = None\n        else: output_len, score_val, *data = de_tokenized\n        if self.formatter is None:\n            assert len(data) == 1\n            decoded = [data[0]]\n        else: decoded = self.formatter.decode_to_array(*data)\n        #if len(decoded)==2:\n        #    same = np.array_equal(decoded[0].get('output'), decoded[1].get('output'))\n        #    print_func(f\"is_identical: {same}\")\n        #    if not same: for i in range(2): print_func(str(decoded[i].get('output')))\n        for d in decoded: d['score_val'] = score_val\n        for i, dec in enumerate(decoded):\n            if i==0: self.process_single_output(key, output_len, dec, print_func=print_func, **kwargs)\n            elif self.additional_decoders:\n                if i-1<len(self.additional_decoders): self.additional_decoders[i-1].process_single_output(key, output_len, dec, print_func=print_func, **kwargs)\n                else: print_func(f'{key} no decoder available for output #{i}')\n            else: self.process_single_output(f'{key}.fix{i}', output_len, dec, print_func=print_func, **kwargs)\n\n    def process(self, key, de_tokenized, **kwargs):\n        for i, d in enumerate(de_tokenized):\n            if self.max_outputs is None or i<=self.max_outputs:\n                self.process_single_decode(f'{key}.out{i}', d, **kwargs)\n\n    def get_unsolved_keys(self):\n        unsolved = []\n        for base_key, reply in self.dataset.replies.items():\n            if not any(np.array_equal(reply[0], s.get('output')) for s in self.decoded_results.get(base_key, {}).values()):\n                unsolved.append(base_key)\n        return unsolved\n\n    def run_selection_algo(self, selection_algorithm):\n        return {bk: (selection_algorithm({k: g for k, g in v.items() if g.get('output') is not None}) if any(g.get('output') is not None for g in v.values()) else []) for bk, v in self.decoded_results.items()}\n\n    def benchmark_selection_algos(self, selection_algorithms, skip_failed=True):\n        import numpy as np\n        results = {}\n        print('*** Benchmark selection algorithms...')\n        for selection_algorithm in selection_algorithms:\n            name = selection_algorithm.__name__\n            try:\n                selected = self.run_selection_algo(selection_algorithm)\n                if self.formatter is not None:\n                    for sols in selected.values():\n                        for s in sols:\n                            assert self.formatter.is_valid_solution(s), f'found invalid solutions {s}'\n                correct_keys = {k for k, v in selected.items() if self.correct_solutions.get(k) is not None and any(np.array_equal(guess, self.correct_solutions[k]) for guess in v[:self.n_guesses])}\n                (score,), score_cnt = self.score(correct_keys)\n                results[name] = score\n                print(f\" acc: {score:5.1f}/{score_cnt:3}={score/score_cnt:6.2%} ('{name}')\")\n            except:\n                print(f\" {'execution failed':>21} ('{name}')\")\n                if not skip_failed: raise\n        return results\n\n    def calc_augmented_scores(self, model, base_keys=None, store=None, seed=0, max_len=None, make_unique=True, quiet=False, **kwargs):\n        if base_keys is None: base_keys = list(self.decoded_results.keys())\n        if store is not None: store = f'{store}_new'  # new format is not backwards compatible, so use new folder\n        for bk in (base_keys if quiet else tqdm(base_keys, desc='calculate augmented scores', file=sys.stdout)):\n            res = self.decoded_results.get(bk, {})\n            known_scores = {}\n            for k, v in sorted(res.items()):\n                if 'output' in v:\n                    k_store = None if store is None else os.path.join(store, k)\n                    id = tuple(map(tuple, v['output']))\n                    if not (make_unique and id in known_scores):\n                        try:\n                            assert k_store is not None\n                            with bz2.BZ2File(k_store) as f: known_scores[id] = pickle.load(f)\n                            if isinstance(known_scores[id], list): known_scores[id] = dict(score_multi=known_scores[id])  # for backwards compatibility\n                            k_store = None\n                        except:\n                            temp_dataset = self.dataset.__class__(\n                                keys=[bk],\n                                queries={bk: self.dataset.queries.get(bk)},\n                                replies={bk: [v['output'].tolist()]},\n                            )\n                            temp_decoder = self.__class__(self.formatter, temp_dataset, n_guesses=self.n_guesses, quiet=True)\n                            temp_dataset = temp_dataset.augment(**kwargs, seed=(seed+hash(k)+hash(id)) % 1024**2, quiet=True)\n                            if max_len is not None: temp_dataset = temp_dataset.cut_to_len(formatter=self.formatter, name='input', max_len=max_len, quiet=True)\n                            for x in temp_dataset.as_list(self.formatter): calc_score(**x, formatter=self.formatter, model=model, decoder=temp_decoder)\n                            known_scores[id] = dict(\n                                score_multi=[np.sum(x['score']) for x in temp_decoder.decoded_results[bk].values()],\n                                score_multi_nl=[x['score_val'] for x in temp_decoder.decoded_results[bk].values()],\n                                score_multi_array=np.array([x['score'] for x in temp_decoder.decoded_results[bk].values()]),\n                                score_multi_array_cum=np.array([x['score_cum'] for x in temp_decoder.decoded_results[bk].values()]),\n                                score_multi_array_all=np.array([x['score_all'] for x in temp_decoder.decoded_results[bk].values()]),\n                                score_multi_array_all_cum=np.array([x['score_all_cum'] for x in temp_decoder.decoded_results[bk].values()]),\n                            )\n                            if k_store is not None:\n                                os.makedirs(store, exist_ok=True)\n                                with bz2.BZ2File(k_store, 'w') as f: pickle.dump(known_scores[id], f)\n                    v.update(known_scores[id])\n\ndef turbo_dfs(model, logits, path, eos_token_id, max_new_tokens, max_score, max_score_greedy, temperature, suppress_tokens, torch, score=0.0, pos=0, cache=None):\n    logits, next_logits = logits[0], (logits[1:] if len(logits)>1 else None)\n    nll = -(logits / temperature).detach().float().log_softmax(-1).cpu().numpy()\n    greedy_index = nll.argmin(-1).item()\n    nll = list(enumerate(nll))\n    if path: nll[0], nll[path[0]], path = nll[path[0]], nll[0], path[1:]  # follow precomputed path first\n    suffixes = []\n    for i, s in nll:\n        next_score = score + s\n        allowed_max_score = max_score_greedy if i==greedy_index else max_score\n        if next_score < allowed_max_score:\n            if i==eos_token_id: next_suffixes = [(next_score, [], [])]\n            elif max_new_tokens>1:\n                if next_logits is None:\n                    if pos<cache[0][0][0].shape[2]: cache[0] = tuple(tuple(c[:, :, :pos] for c in l) for l in cache[0])\n                    next_logits, cache[0] = model(\n                        input_ids= torch.full((1,1), i, device=model.device),\n                        position_ids=torch.full((1,1), pos, device=model.device),\n                        past_key_values=cache[0],\n                    )[:2]\n                    next_logits = next_logits[0]  # unbatch\n                next_suffixes = turbo_dfs(model, logits=next_logits, path=path, eos_token_id=eos_token_id, max_new_tokens=max_new_tokens-1, max_score=max_score, max_score_greedy=allowed_max_score, temperature=temperature, suppress_tokens=suppress_tokens, torch=torch, score=next_score, pos=pos+1, cache=cache)\n            else: next_suffixes = []\n            for suffix in next_suffixes:\n                suffix[1].append(i)\n                suffix[2].append(logits)\n            suffixes.extend(next_suffixes)\n        next_logits = None\n    return suffixes\n\ndef inference_turbo_dfs(model, input_ids, eos_token_id, max_new_tokens, min_prob, min_prob_greedy=1, temperature=1.0, suppress_tokens=[], path=[], attention_mask=None):\n    import torch\n    with torch.no_grad():\n        assert attention_mask is None or attention_mask.all(), 'not implemented'\n        input_ids = torch.as_tensor(input_ids, device=model.device, dtype=int)\n        if input_ids.ndim==2: input_ids = input_ids.squeeze(0)\n        assert input_ids.ndim==1, 'batching not supported'\n        max_score = -np.log(min_prob)\n        max_score_greedy = (-np.log(min_prob_greedy)) if min_prob_greedy>0 else float('inf')  # avoid throwing numpy error\n        max_score_greedy = max(max_score, max_score_greedy)\n        if path is None: path = []\n        if len(path) and path[-1]==eos_token_id: path = path[:-1]\n        with torch.no_grad():\n            full_path = input_ids\n            if len(path): full_path = torch.cat([full_path, torch.as_tensor(path, device=model.device)])\n            logits, cache = model(input_ids=full_path[np.newaxis])[:2]\n            logits = logits[0, len(input_ids)-1:]\n        result = turbo_dfs(model, logits=logits, path=path, eos_token_id=eos_token_id, max_new_tokens=max_new_tokens, max_score=max_score, max_score_greedy=max_score_greedy, temperature=temperature, suppress_tokens=suppress_tokens, torch=torch, score=0.0, pos=len(input_ids), cache=[cache])\n        return sorted([(score_val, np.array(suffix[::-1]), torch.stack(score_arr[::-1]).float().cpu().numpy()) for score_val, suffix, score_arr in result], key=lambda x:x[0])\n\ndef inference_step(tokenized, model, remove_token_type_ids=True, num_beams=1, formatter=None, min_prob=None, current_best=None, **kwargs):\n    import torch\n    if remove_token_type_ids: tokenized.pop('token_type_ids', None)\n    if min_prob is not None:\n        assert num_beams==1\n        gen = inference_turbo_dfs(model, **tokenized.to(model.device), path=current_best, min_prob=min_prob, eos_token_id=formatter.tokenizer.eos_token_id, **kwargs)\n        tokens_out = [[g[1] for g in gen]]\n        scores_out = [[g[2] for g in gen]]\n    elif is_unsloth_model(model) and num_beams > 1:\n        assert False, 'unsloth does not support beam search'\n    else:\n        gen = model.generate(**tokenized.to(model.device), return_dict_in_generate=True, output_logits=True, use_cache=True, **kwargs)\n        tokens_out = gen['sequences'][:, torch.newaxis, tokenized['input_ids'].shape[-1]:].cpu().numpy().copy()\n        scores_out = torch.stack(gen['logits'], axis=-2)[:, torch.newaxis].float().cpu().numpy().copy()\n    return tokens_out, scores_out\n\ndef process_inference_output(key, outputs, formatter, store=None, decoder=None, decoder_args={}):\n    de_tokenized = [formatter.de_tokenize(*output) for output in zip(*outputs)]\n    inference_save(store, key, de_tokenized)\n    if decoder is not None: decoder.process(key, de_tokenized, **decoder_args)\n    return de_tokenized\n\ndef inference_run_v2(model, formatter, dataset, decoder=None, max_new_tokens=None, max_batch_size=1, store=None, result_dict=None, rerun_empty=False, retrain=None, use_turbo=False, group_multi_output=True, **kwargs):\n    import torch\n    assert max_batch_size==1, 'unsupported'\n\n    with torch.no_grad():\n        print('*** Load stored data...')\n        if result_dict is None: result_dict = {}\n        result_dict = inference_load(store, dataset.keys, result_dict)\n        by_base_key = {}\n        needs_rerun = {}\n        base_key_list = []\n        for key in dataset.keys:\n            base_key = key.split('.')[0]\n            if group_multi_output: base_key = base_key.split('_')[0]\n            if base_key not in by_base_key: base_key_list.append(base_key)\n            bk_list = by_base_key[base_key] = by_base_key.get(base_key, [])\n            bk_list.append(key)\n        for base_key, keys in by_base_key.items():\n            for key in keys:\n                de_tokenized = result_dict.get(key)\n                if de_tokenized is None or (rerun_empty and not de_tokenized):\n                    bk_list = needs_rerun[base_key] = needs_rerun.get(base_key, [])\n                    bk_list.append(key)\n                elif decoder is not None: decoder.process(key, de_tokenized)\n\n        formatter.tokenizer.padding_side = 'left'\n        if max_new_tokens is None: max_new_tokens = formatter.max_new_tokens()\n        if is_unsloth_model(model):\n            from unsloth import FastLanguageModel\n            FastLanguageModel.for_inference(model)\n        else: model.eval()\n\n        print('*** Start inference run...')\n    try:\n        with tqdm(base_key_list, file=sys.stdout) as pbar:\n            for base_key in pbar:\n                run_keys = needs_rerun.get(base_key)\n                if run_keys:\n                    if retrain is not None:\n                        retrain_dataset = dataset.keep_key_startswith(base_key)\n                        print(f\"retraining model for key '{base_key}' (retrain_dataset_size={len(retrain_dataset.keys)})\")\n                        retrain(model, retrain_dataset)\n                        if is_unsloth_model(model): FastLanguageModel.for_inference(model)\n                    with torch.no_grad():\n                        for key in run_keys:\n                            input_text = dataset.get(key, formatter)['input']\n                            batch = formatter.tokenizer([input_text], return_tensors='pt')\n                            current_best = decoder.get_current_best(key.split('.')[0]) if use_turbo else None\n                            if current_best is not None:\n                                current_best = dataset.forward_mod(current_best, key)\n                                current_best = formatter.fmt_reply([current_best])\n                                current_best = formatter.tokenizer(input_text+current_best)['input_ids'][batch['input_ids'].shape[-1]:]\n                            batch_out = inference_step(batch, model, formatter=formatter, max_new_tokens=max_new_tokens, current_best=current_best, **kwargs)\n                            outputs = [x[0] for x in batch_out]\n                            result_dict[key] = process_inference_output(key, outputs, formatter, store=store, decoder=decoder, decoder_args=dict(print_func=pbar.write))\n        print('*** Completed inference run.')\n    except KeyboardInterrupt: print('*** Ctrl+C pressed, stopping inference run.')\n    return result_dict\n\nclass Retrainer(object):\n    def __init__(self, n, aug_opts, reload_state_dict=None, **kwargs):\n        self.n = n\n        self.aug_opts = aug_opts\n        self.reload_state_dict = reload_state_dict\n        self.kwargs = kwargs\n\n    def preprocess(self, dataset):\n        ds = [dataset.augment(quiet=True, shfl_keys=True, **self.aug_opts) for _ in range((self.n-1)//dataset.length()+1)]\n        ds = ds[0] if len(ds)==1 else ds[0].append(*ds[1:])\n        ds, _ = ds.split_at_pos(self.n)\n        return ds\n\n    def __call__(self, model, dataset):\n        if self.reload_state_dict is not None: set_peft_weights(model, self.reload_state_dict)\n        assert is_unsloth_model(model), 'not implemented'\n        if is_unsloth_model(model):\n            from unsloth import FastLanguageModel\n            FastLanguageModel.for_training(model)\n        else: model.train()\n        training_run(model, dataset=self.preprocess(dataset), **self.kwargs)\n\ndef calc_score(key, input, reply, formatter, model, store=None, decoder=None, **_):\n    import torch\n    with torch.no_grad():\n        input_len = len(formatter.tokenizer(input)['input_ids'])\n        tokenized = formatter.tokenizer([input+reply], return_tensors='pt')\n        reply_tok = tokenized['input_ids'][0][input_len:].cpu().numpy().copy()\n        reply_log = model.forward(**tokenized.to(model.device))['logits'][0, input_len-1: -1].float().cpu().numpy().copy()\n        process_inference_output(key, (reply_tok[torch.newaxis], reply_log[torch.newaxis]), formatter, store=store, decoder=decoder)\n\ndef mem_info(gpu_id=0):\n    import torch\n    try:\n        gpu_stats = torch.cuda.get_device_properties(gpu_id)\n        usage = torch.cuda.max_memory_reserved() / 1024**3\n        avail = gpu_stats.total_memory / 1024**3\n        print(f\"*** GPU: {gpu_stats.name}, used {usage:.3} / {avail:.3} GB.\")\n    except: print('*** Exception occured when getting memory stats.')","metadata":{"execution":{"iopub.execute_input":"2025-04-04T13:51:17.59732Z","iopub.status.busy":"2025-04-04T13:51:17.597103Z","iopub.status.idle":"2025-04-04T13:51:17.621043Z","shell.execute_reply":"2025-04-04T13:51:17.620488Z"},"papermill":{"duration":0.033305,"end_time":"2025-04-04T13:51:17.622433","exception":false,"start_time":"2025-04-04T13:51:17.589128","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ARC Dataset Processing and Formatting Library\n\nThis code defines a comprehensive library for working with the Abstraction and Reasoning Corpus (ARC) dataset, providing utilities for data loading, manipulation, augmentation, and formatting for machine learning models. Here's an explanation of the main components:\n\n## Core Functionality\n\n1. **Data Manipulation Utilities**\n   - `cut_at_token`: Truncates arrays at specified token positions\n   - `shuffled`: Randomizes array elements with NumPy's permutation\n   - `permute_mod`: Applies number permutations to arrays with inversion control\n   - Various permutation strategies (random, frequency-based) for data augmentation\n\n2. **ArcDataset Class**\n   - Handles loading and processing of ARC challenge datasets\n   - Provides comprehensive dataset manipulation methods:\n     - Data augmentation (rotation, transposition, permutation)\n     - Task filtering and sorting\n     - Example shuffling and selection\n     - Dataset splitting and concatenation\n   - Includes utilities for submission creation and validation\n\n3. **ArcFormatter Class**\n   - Converts grid-based ARC tasks into text format for language models\n   - Configurable formatting with options for prefixes, separators, and tokenization\n   - Handles decoding model outputs back into valid grid solutions\n   - Supports scoring and evaluation of predictions\n   - Includes methods for formatting train-test examples and queries\n\n4. **Custom Data Collator**\n   - Implements special handling for training language models on ARC tasks\n   - Supports advanced techniques like output masking and controlled fault injection\n   - Configurable through options like `fault_freq` and `mask_first_output`\n\n## Key Features\n\n- **Data Augmentation**: Extensive options for task transformation to increase training data variety\n- **Formatting Flexibility**: Customizable text representations for different model preferences\n- **Length Management**: Methods to filter and truncate tasks to fit model context windows\n- **Submission Handling**: Tools for generating and validating competition submissions\n- **Predefined Formatters**: Ready-to-use configurations like `ArcFormatter_pretext2` with different masking strategies\n\nThis library provides the infrastructure needed to process ARC tasks for machine learning models, handling the conversion between grid-based puzzle representations and the text formats needed by language models.","metadata":{"papermill":{"duration":0.006615,"end_time":"2025-04-04T13:51:17.635906","exception":false,"start_time":"2025-04-04T13:51:17.629291","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%%writefile arc_loader.py\nimport json\nimport numpy as np\nimport hashlib\nimport os, sys\nfrom tqdm import tqdm\nfrom glob import glob\nimport itertools\nimport random\n\ndef cut_at_token(output, token_id):\n    eos_positions = (output==token_id).nonzero()[0]\n    return output[:eos_positions[0]] if len(eos_positions) else output\n\ndef shuffled(data_list):\n    return np.random.permutation(data_list).tolist()\n\ndef permute_mod(a, descriptor, invert=False):\n    permutation = [int(i) for i in descriptor if str(i).isdigit()]\n    assert sorted(permutation)==list(range(10))\n    a = np.asarray(a)\n    if a.ndim==3:\n        if not invert: permutation = np.argsort(permutation)\n        a = a[..., permutation]\n    else:\n        assert a.ndim==2\n        if invert: permutation = np.argsort(permutation)\n        a = np.asarray(permutation)[a]\n    return a\n\ndef permute_rnd_col_(query):\n    permutation = [0]+(1+np.random.permutation(9)).tolist()\n    return 'permute' + ''.join(map(str, permutation))\n\ndef permute_rnd_all_(query):\n    permutation = np.random.permutation(10).tolist()\n    return 'permute' + ''.join(map(str, permutation))\n\ndef permute_cnt_col_(query):\n    elements, frequency = np.unique(np.concatenate([list(range(10))]+[np.array(x['input']).ravel() for x in query['train']]), return_counts=True)\n    permutation = [0]+sorted(np.random.permutation(9)+1, key=lambda i: frequency[i], reverse=True)  # randomness as tie breaker\n    return 'permute' + ''.join(map(str, permutation))\n\ndef permute_cnt_all_(query):\n    elements, frequency = np.unique(np.concatenate([list(range(10))]+[np.array(x['input']).ravel() for x in query['train']]), return_counts=True)\n    permutation = sorted(np.random.permutation(10), key=lambda i: frequency[i], reverse=True)  # randomness as tie breaker\n    return 'permute' + ''.join(map(str, permutation))\n\npermute_rnd_col = (permute_mod, permute_rnd_col_)\npermute_rnd_all = (permute_mod, permute_rnd_all_)\npermute_cnt_col = (permute_mod, permute_cnt_col_)\npermute_cnt_all = (permute_mod, permute_cnt_all_)\npermute_None = (np.copy, None)\n\nclass ArcDataset(object):\n    @staticmethod\n    def forward_mod(a, key, use_perm=True, is_output=True):\n        if a is None: return a\n        for op in key.split('.')[1:]:\n            if op.startswith('I'):\n                if is_output: continue\n                op = op[1:]\n            if   op=='rot90':              a = np.rot90(a)\n            elif op=='transpose':          a = np.swapaxes(a, 0, 1)\n            elif op.startswith('permute'): a = permute_mod(a, op, invert=False) if use_perm else a\n            elif op.startswith('copy'):    a = np.copy(a)\n            elif op.startswith('out'):     a = a\n            elif op.startswith('ex'):      a = a\n            elif op.startswith('fix'):     a = a\n            elif op.startswith('ice'):     a = a  # for adding icecuber solutions\n            else: raise NotImplementedError(f\"Inversion of operation '{op}' unknown.\")\n        return a\n\n    @staticmethod\n    def invert_mod(a, key, inv_perm=True, is_output=True):\n        if a is None: return a\n        for op in key.split('.')[1:][::-1]:\n            if op.startswith('I'):\n                if is_output: continue\n                op = op[1:]\n            if   op=='rot90':              a = np.rot90(np.rot90(np.rot90(a)))\n            elif op=='transpose':          a = np.swapaxes(a, 0, 1)\n            elif op.startswith('permute'): a = permute_mod(a, op, invert=True) if inv_perm else a\n            elif op.startswith('copy'):    a = np.copy(a)\n            elif op.startswith('out'):     a = a\n            elif op.startswith('ex'):      a = a\n            elif op.startswith('fix'):     a = a\n            elif op.startswith('ice'):     a = a  # for adding icecuber solutions\n            else: raise NotImplementedError(f\"Inversion of operation '{op}' unknown.\")\n        return a\n\n    def __init__(self, queries, replies={}, keys=None, is_orig=False, is_fake=False):\n        if keys is not None: keys = [k for k in keys if k is not None]\n        self.queries = queries if keys is None else {k: queries[k] for k in keys}\n        self.replies = replies if keys is None else {k: replies[k] for k in keys if k in replies}\n        self.is_orig = is_orig\n        self.is_fake = is_fake\n        self.keys = sorted(queries.keys()) if keys is None else keys\n        self.faulty = {}\n        self.transposed_dataset = None\n\n    @classmethod\n    def empty(cls):\n        return cls(queries={}, replies={}, keys=[])\n\n    def change_keys(self, keys, keep_flags=False):\n        flags = dict(is_fake=self.is_fake, is_orig=self.is_orig) if keep_flags else {}\n        return self.__class__(queries=self.queries, replies=self.replies, keys=keys, **flags)\n\n    @classmethod\n    def from_file(cls, queries_file):\n        print(f\"*** Load challanges from '{queries_file}'...\")\n        with open(queries_file) as f: queries = f.read()\n        import os\n        if os.getenv('KAGGLE_IS_COMPETITION_RERUN'): #Real submit\n            is_fake = False\n        else: #Fake run\n            is_fake = True\n        #is_fake = hashlib.md5(queries.encode('utf-8')).hexdigest().lower()=='a6b7dac3cab03abf2eb333e16610d6dc'\n        if is_fake: print(\"*** -> Fake test set detected, setting flag 'is_fake' to True.\")\n        return cls(\n            queries=json.loads(queries),\n            is_fake=is_fake,\n            is_orig=True,\n        )\n\n    def load_replies(self, replies_file):\n        print(f\"*** Load solutions from '{replies_file}'...\")\n        with open(replies_file) as f: replies = f.read()\n        replies_parsed = json.loads(replies)\n        self.replies = {k: replies_parsed[k] for k in self.keys}\n        return self\n\n    def split_multi_replies(self):\n        key_indices = [(k, i) for k in self.keys for i in range(len(self.queries[k]['test']))]\n        return self.__class__(\n            keys=[f'{k}_{i}' for k, i in key_indices],\n            queries={f'{k}_{i}': {'train': self.queries[k]['train'], 'test': [self.queries[k]['test'][i]]} for k, i in key_indices},\n            replies={f'{k}_{i}': [self.replies[k][i]] for k, i in key_indices if k in self.replies},\n        )\n\n    def move_test_to_train(self):\n        new_queries = {k: {'train': self.queries[k]['train'] + [{**t, 'output': self.replies[k][i]} for i, t in enumerate(self.queries[k]['test'])], 'test': []} for k in self.keys}\n        return self.__class__(queries=new_queries, keys=[k for k in self.keys])\n\n    def last_train_ex_for_test(self):\n        assert not self.replies\n        new_queries = {k: {'train': self.queries[k]['train'][:-1], 'test': [{'input': self.queries[k]['train'][-1]['input']}]} for k in self.keys}\n        new_replies = {k: [self.queries[k]['train'][-1]['output']] for k in self.keys}\n        return self.__class__(queries=new_queries, replies=new_replies, keys=[k for k in self.keys])\n\n    def length(self):\n        return len(self.keys)\n\n    def shuffled(self, seed=None):\n        if seed is not None: np.random.seed(seed)\n        return self.__class__(queries=self.queries, replies=self.replies, keys=shuffled(self.keys))\n\n    def sorted(self, **kwargs):\n        return self.__class__(queries=self.queries, replies=self.replies, keys=sorted(self.keys, **kwargs))\n\n    def append(*datasets):\n        return datasets[0].__class__(\n            queries={k: v for d in datasets for k, v in d.queries.items()},\n            replies={k: v for d in datasets for k, v in d.replies.items()},\n            keys   =[k    for d in datasets for k    in d.keys           ],\n        )\n\n    def sort_ex_by_input_size(self, seed=42, reverse=False):\n        np.random.seed(seed)\n        sort_key = lambda ex: np.prod(np.shape(ex['input']))\n        new_queries = {k2: {k: (sorted(np.random.permutation(np.array(v, dtype=object)), key=sort_key, reverse=reverse) if k=='train' else v) for k, v in v2.items()} for k2, v2 in self.queries.items()}\n        return self.__class__(queries=new_queries, replies=self.replies, keys=[k for k in self.keys])\n\n    def interleave(self, block_size, num_gpus=None):\n        keys = np.reshape(self.keys, (-1, block_size)).T\n        if num_gpus is None: return self.change_keys(keys.ravel().tolist())\n        ret, num_gpus = (None, num_gpus) if isinstance(num_gpus, int) else num_gpus\n        keys = np.concatenate([keys, np.full((-keys.shape[0]%num_gpus, keys.shape[1]), None)])\n        keys = np.reshape(keys, (keys.shape[0]//num_gpus, num_gpus, -1)).swapaxes(0, 1).reshape(num_gpus, -1)\n        new_datasets = [self.change_keys(gpu_keys.tolist()) for gpu_keys in keys]\n        return new_datasets if ret is None else new_datasets[ret]\n\n    def remove(self, *datasets):\n        remove_keys = {k for d in datasets for k in d.keys}\n        new_keys = [k for k in self.keys if k not in remove_keys]\n        return self.change_keys(new_keys)\n\n    def keep_key_startswith(self, key_start):\n        new_keys = [k for k in self.keys if k.startswith(key_start)]\n        return self.change_keys(new_keys)\n\n    def mod_single(self, mod_func, descriptor, i, keep_key, inputs_only):\n        queries = {}\n        replies = {}\n        keys    = []\n        for k0 in self.keys:\n            desc = (('copy{i}' if mod_func is np.copy else mod_func.__name__) if descriptor is None else descriptor if isinstance(descriptor, str) else descriptor(self.queries[k0])).format(i=i)\n            func = lambda a, d: np.asarray(mod_func(a) if descriptor is None else mod_func(a, d)).tolist()\n            k1 = k0 if keep_key else f\"{k0}.{'I' if inputs_only else ''}{desc}\"\n            keys.append(k1)\n            queries[k1] = {m: [{t: (func(a, desc) if t=='input' or not inputs_only else a) for t, a in x.items()} for x in e] for m, e in self.queries[k0].items()}\n            if k0 in self.replies:\n                replies[k1] = [func(a, desc) for a in self.replies[k0]]\n        ret = self.__class__(queries=queries, replies=replies, keys=keys)\n        return ret\n\n    def mod(self, mod_func, descriptor=None, n=1, stack=None, keep=False, keep_key=False, shuffle=False, join=True, inputs_only=False):\n        assert not (keep and keep_key)\n        cur = self\n        ret = [cur.shuffled() if shuffle else cur] if keep else []\n        if stack is None: stack = mod_func.__name__.startswith('rot')\n        for i in range(n):\n            cur = (cur if stack else self).mod_single(mod_func, descriptor, i=i, keep_key=keep_key, inputs_only=inputs_only)\n            ret.append(cur.shuffled() if shuffle else cur)\n        return self.__class__.append(*ret) if join else ret\n\n    def get(self, key, formatter):\n        assert formatter.out2_token is None or key in self.replies\n        train = formatter.fmt_train(self.queries[key]['train'])\n        query = formatter.fmt_query(self.queries[key]['test'], i=len(self.queries[key]['train']))\n        reply = formatter.fmt_reply(self.replies[key], self.faulty.get(key)) if key in self.replies else ''\n        text = train+query+reply if reply else formatter.fmt_train(self.queries[key]['train'], last_is_challenge=True)\n        return dict(key=key, train=train, query=query, reply=reply, input=train+query, text=text)\n\n    def as_list(self, formatter):\n        return [self.get(key, formatter) for key in self.keys]\n\n    def as_dataset(self):\n        from datasets import Dataset\n        return Dataset.from_list([{'key': k, 'query': self.queries[k], 'reply': self.replies[k]} for k in self.keys])\n\n    def get_length(self, key, formatter, name, max_of_transposed=False):\n        if formatter is None:\n            if   name=='input': return sum(np.prod(np.shape(v)) for v3 in self.queries[key].values() for v2 in v3 for v in v2.values())\n            elif name=='reply': return sum(np.prod(np.shape(v)) for v in self.replies[key])\n            else: assert False\n        else:\n            datasets = [self]\n            if max_of_transposed:\n                if self.transposed_dataset is None: self.transposed_dataset = self.mod(np.transpose, keep=False, keep_key=True)\n                datasets.append(self.transposed_dataset)\n            return max(len(formatter.tokenizer(ds.get(key, formatter=formatter)[name])['input_ids']) for ds in datasets)\n\n    def get_lengths(self, formatter, name, max_of_transposed=False):\n        return {key: self.get_length(key, formatter=formatter, name=name, max_of_transposed=max_of_transposed) for key in self.keys}\n\n    def sorted_by_len(self, reverse=False, **kwargs):\n        new_keys = [key for _, key in sorted([(v, k) for k, v in self.get_lengths(**kwargs).items()], reverse=reverse)]\n        return self.change_keys(new_keys)\n\n    def filter_by_len(self, min_len=0, max_len=float('inf'), **kwargs):\n        new_keys = [k for k, v in self.get_lengths(**kwargs).items() if min_len<=v<=max_len]\n        return self.change_keys(new_keys)\n\n    def cut_to_query_count(self, max_count, from_end=False):\n        new_queries = {}\n        for k in self.keys:\n            new_queries[k] = q = self.queries[k]\n            while len(q['train'])>max_count: q['train'] = q['train'][:-1] if from_end else q['train'][1:]\n        return self.__class__(queries=new_queries, replies=self.replies, keys=[k for k in self.keys])\n\n    def cut_to_len(self, formatter, name, max_len, max_new_tokens='auto', from_end=False, quiet=False, **kwargs):\n        if max_new_tokens:\n            if max_new_tokens=='auto': max_new_tokens = formatter.max_new_tokens()\n            max_len_old, max_len = max_len, max_len - max_new_tokens\n            if not quiet: print(f'*** Reducing task size to max. {max_len_old} tokens ({max_len} input + {max_new_tokens} generated)...')\n        elif not quiet: print(f'*** Reducing task size to max. {max_len} tokens...')\n        temp_ds = self.change_keys(self.keys)\n        new_keys = []\n        new_queries = {}\n        new_replies = {}\n        for key in (self.keys if quiet else tqdm(self.keys, file=sys.stdout)):\n            reply = temp_ds.replies.get(key)\n            while max_len<temp_ds.get_length(key, formatter=formatter, name=name, **kwargs):\n                query = temp_ds.queries[key]\n                if not key.split('.')[-1].startswith('ex'): key = f\"{key}.ex{''.join(map(str, range(len(query['train']))))}\"\n                key_split = key.split('.')\n                assert key_split[-1].startswith('ex')\n                key = '.'.join(key_split[:-1] + [f'ex{key_split[-1][2:-1] if from_end else key_split[-1][3:]}'])\n                temp_ds.queries[key] = {k: ((v[:-1] if from_end else v[1:]) if k=='train' else v) for k, v in query.items()}\n                if reply is not None: temp_ds.replies[key] = reply\n            new_keys.append(key)\n            new_queries[key] = temp_ds.queries[key]\n            if reply is not None: new_replies[key] = reply\n        return self.__class__(keys=new_keys, queries=new_queries, replies=new_replies)\n\n    def shuffle_ex(self, perm=None, keep_max=None):\n        new_keys = []\n        new_queries = {}\n        new_replies = {}\n        for key in self.keys:\n            n = len(self.queries[key]['train'])\n            p = np.random.permutation(n) if perm is None else perm\n            if keep_max is not None: p = p[:keep_max]\n            new_key = f'{key}.ex' + ('-' if (p.max()>9) else '').join(map(str, p.tolist()))\n            new_keys.append(new_key)\n            new_queries[new_key] = {k: (np.array(v, dtype=object)[p].tolist() if k=='train' else v) for k, v in self.queries[key].items()}\n            if key in self.replies: new_replies[new_key] = self.replies[key]\n        return self.__class__(queries=new_queries, replies=new_replies, keys=new_keys)\n\n    def shuffle_rp(self, keep_max=None):\n        new_keys = []\n        new_queries = {}\n        new_replies = {}\n        for key in self.keys:\n            n = len(self.queries[key]['test'])\n            p = np.random.permutation(n)\n            if keep_max is not None: p = p[:keep_max]\n            new_key = f'{key}.rp' + ('-' if (p.max()>9) else '').join(map(str, p.tolist()))\n            new_keys.append(new_key)\n            new_queries[new_key] = {k: (np.array(v, dtype=object)[p].tolist() if k=='test' else v) for k, v in self.queries[key].items()}\n            if key in self.replies: new_replies[new_key] = np.array(self.replies[key], dtype=object)[p].tolist()\n        return self.__class__(queries=new_queries, replies=new_replies, keys=new_keys)\n\n    def append_to_keys(self, test):\n        return self.change_keys([f'{k}{text}' for k in self.keys])\n\n    def random_select(self, n):\n        keys = np.array(self.keys).reshape(n, -1).T\n        choice = np.random.randint(0, n, size=[len(keys)])\n        return self.change_keys(keys[np.arange(len(keys)), choice])\n\n    def augment(self, tp=False, rot=False, n=1, perm=None, perm_append=False, shfl_keys=False, shfl_ex=False, seed=None, quiet=False, inputs_only=False):\n        if not quiet: print(f\"*** Augment dataset{' (inputs only)' if inputs_only else ''}...\")\n        np.random.seed(seed)\n        d = self\n        if tp: d = d.mod(np.transpose, keep=True, inputs_only=inputs_only)\n        if tp=='rand': d = d.random_select(n=2)\n        if rot: d = d.mod(np.rot90, n=3, keep=True, inputs_only=inputs_only)\n        if rot=='rand': d = d.random_select(n=4)\n        if perm is None and n<=1: d = d.shuffled() if shfl_keys else d\n        else: d = d.mod(*([np.copy] if perm is None else globals()[f\"permute_{perm}\"]), n=n, shuffle=shfl_keys, keep=perm_append, inputs_only=inputs_only)\n        np.random.seed(seed)\n        if shfl_ex: d = d.shuffle_ex()\n        return d\n\n    def remove_replies(self):\n        return self.__class__(queries=self.queries, replies={}, keys=[k for k in self.keys])\n\n    def split_at_pos(self, pos, random_seed=None):\n        keys = self.keys\n        if random_seed is not None:\n            np.random.seed(random_seed)\n            keys = np.random.permutation(keys)\n        if isinstance(pos, float): pos = int(pos * len(self.keys) + 0.5)\n        keys_split = [keys[:pos], keys[pos:]]\n        return tuple(self.change_keys(new_keys, keep_flags=True) for new_keys in keys_split)\n\n    def get_submission(self, results=None):\n        assert self.is_orig==True, 'Must be run on original dataset.'\n        submission = {k: [{f'attempt_{i+1}': [[0]] for i in range(2)} for _ in range(len(self.queries[k]['test']))] for k in self.keys}\n        if results is not None: self.fill_submission(results, submission)\n        return submission\n\n    @staticmethod\n    def fill_submission(results, submission):\n        print(f'*** Generating submission for {len(results)} outputs...')\n        for k, v in results.items():\n            base_id, base_nr = k.split('_')\n            target_dict = submission[base_id][int(base_nr)]\n            for i, g in enumerate(v[:len(target_dict)]):\n                target_dict[f'attempt_{i+1}'] = g.tolist()\n\n    def validate_submission(self, submission):\n        assert self.is_orig==True, 'Must be run on original dataset.'\n        score = 0\n        for k, v in self.replies.items():\n            for i, r in enumerate(v):\n                for attempt in ['attempt_1', 'attempt_2']:\n                    if np.array_equal(r, submission[k][i][attempt]):\n                        score += 1 / len(v)\n                        break\n        return score\ndef get_class_MyDataCollator(cache=[]):\n    if not cache:\n        from trl import DataCollatorForCompletionOnlyLM\n        class MyDataCollator(DataCollatorForCompletionOnlyLM):\n            def setup(self, out2_token_id=None, fault_token_id=None, fault_freq=0, sample_tries=8, mask_first_output=False):\n                self.out2_token_id = out2_token_id\n                self.fault_token_id = fault_token_id\n                self.fault_freq = fault_freq\n                self.sample_tries = sample_tries\n                self.mask_first_output = mask_first_output\n                return self\n\n            def torch_call(self, examples):\n                batch = super().torch_call(examples)\n                if self.out2_token_id is not None:\n                    assert not self.fault_freq\n                    for i in range(len(batch['input_ids'])):\n                        end_pos = ((batch['labels'][i] != -100              ).nonzero().max()).item() + 1\n                        mid_pos = ((batch['labels'][i] == self.out2_token_id).nonzero().max()).item() + 1\n                        beg_pos = mid_pos - (end_pos - mid_pos)\n                        batch['labels'][i][beg_pos:mid_pos] = batch['labels'][i][mid_pos:end_pos]\n                elif self.fault_freq:\n                    for i in range(len(batch['input_ids'])):\n                        end_pos = ((batch['labels'][i] != -100).nonzero().max()).item() + 1\n                        if not isinstance(self.fault_freq, float):\n                            eos_token_id = batch['labels'][i][end_pos - 1]\n                            num_examples = (batch['labels'][i] == eos_token_id).sum().item() - 1\n                            fault_freq = self.fault_freq[num_examples]\n                        else: fault_freq = self.fault_freq\n                        if random.random() < fault_freq:\n                            beg_pos = ((batch['labels'][i][:end_pos]==-100).nonzero().max()).item() + 1\n                            fault_pos = random.randint(beg_pos, end_pos-2)\n                            fault_tok = batch['labels'][i][fault_pos].item()\n                            for t in range(self.sample_tries):\n                                new_tok = batch['labels'][i][random.randint(beg_pos, end_pos-2)].item()\n                                if fault_tok!=new_tok:\n                                    batch['input_ids'][i][fault_pos] = new_tok\n                                    batch['labels'][i][fault_pos+1:end_pos] = self.fault_token_id\n                                    break\n                for i in range(len(batch['labels'])):\n                    for _ in range(self.mask_first_output):\n                        beg_pos = ((batch['labels'][i] != -100).nonzero().min()).item()\n                        mid_pos = ((batch['labels'][i][beg_pos:] == -100).nonzero().min()).item() + beg_pos\n                        end_pos = ((batch['labels'][i] != -100).nonzero().max()).item() + 1\n                        if mid_pos<end_pos: batch['labels'][i][beg_pos:mid_pos] = -100\n                return batch\n        cache.append(MyDataCollator)\n    return cache[0]\n\nclass ArcFormatter(object):\n    def __init__(self, inp_prefix, out_prefix, arr_sep, out2_use=False, out2_token=None, arr_beg='', arr_end='', pretext='', pre_out=None, exa_sep='', exa_end='', qry_prefix=None, rpl_prefix=None, rpl_sep=None, dec_sep=None, min_wid=0, min_pad='', pretext_corpus_split='', masking=0, tokenizer=None, collator_kwargs={}, repeat_input_aug=None, repeat_input_pre=None):\n        self.tokenizer = tokenizer\n        self.inp_prefix = inp_prefix\n        self.out_prefix = out_prefix\n        self.out2_token = out2_token\n        self.out2_use = out2_use\n        assert not out2_use or out2_token is not None\n        assert not out2_use or masking in [1, 2]\n        assert masking!=2 or out2_use or rpl_prefix is not None\n        self.qry_prefix = qry_prefix if qry_prefix is not None else inp_prefix\n        self.rpl_prefix = rpl_prefix if rpl_prefix is not None else out_prefix\n        self.rpl_sep = rpl_sep if rpl_sep is not None else self.rpl_prefix\n        self.arr_sep = arr_sep\n        self.arr_beg = arr_beg\n        self.arr_end = arr_end\n        self.pretext = pretext\n        self.pre_out = pre_out\n        self.pre_out_empty = ['']*99\n        self.pretext_corpus_split = pretext_corpus_split\n        self.exa_sep = exa_sep\n        self.exa_end = exa_end\n        self.dec_sep = arr_sep if dec_sep is None else dec_sep\n        self.min_wid = min_wid\n        self.min_pad = min_pad\n        self.masking = masking\n        self.collator_kwargs = collator_kwargs\n        self.repeat_input_aug = repeat_input_aug\n        self.repeat_input_pre = repeat_input_pre\n\n    def fmt_array(self, array):\n        return self.arr_beg + self.arr_sep.join(str(row).replace(' ', '').replace(',', '').replace('[', '').replace(']', '')+self.min_pad*max(0, self.min_wid-len(row)) for row in array) + self.arr_end\n\n    def get_pre_out(self, pretext_split):\n        if self.pre_out is None: return self.pre_out_empty\n        if pretext_split: return [self.pretext_corpus_split.join(list(p) + ['']) for p in self.pre_out]\n        return self.pre_out\n\n    def fmt_train(self, train, last_is_challenge=False, pretext_split=False):\n        po = self.get_pre_out(pretext_split=pretext_split)\n        ex = [(f\"{self.fmt_query([x], i, pretext_split=pretext_split)}{self.fmt_reply([x['output']])}\" if last_is_challenge and i+1==len(train) else\n               f\"{self.inp_prefix}{self.fmt_array(x['input'])}{self.repeat_input(x, no_aug=pretext_split)}{po[i]}{self.out_prefix}{self.fmt_array(x['output'])}\") for i, x in enumerate(train)]\n        pre = self.pretext_corpus_split.join(list(self.pretext)+['']) if pretext_split else self.pretext\n        end = '' if last_is_challenge else (self.exa_end + self.tokenizer.eos_token)\n        return pre + (self.exa_end + self.tokenizer.eos_token + self.exa_sep).join(ex) + end\n\n    def fmt_query(self, query, i, pretext_split=False):\n        po = self.get_pre_out(pretext_split=pretext_split)\n        return ''.join(f\"{self.qry_prefix}{self.fmt_array(x['input'])}{self.repeat_input(x, no_aug=pretext_split)}{po[i]}{self.rpl_prefix}\" for x in query[:1])\n\n    def repeat_input(self, x, no_aug=False):\n        if self.repeat_input_aug is None: return ''\n        return f\"{self.repeat_input_pre}{self.fmt_array(((lambda x: x) if no_aug else self.repeat_input_aug)(x['input']))}\"\n\n    def fmt_reply(self, reply, fault=None):\n        ids = self.fmt_array(reply[0]) + self.exa_end + self.tokenizer.eos_token\n        if self.out2_use:\n            if fault is None: fault = reply\n            ids = self.fmt_array(fault[0]) + self.exa_end + self.out2_token + ids\n        return ids\n\n    def quick_test(self, decoded, done):\n        sp = decoded.split(self.tokenizer.eos_token)[0].split(self.dec_sep)\n        sl = len(sp[0])\n        is_prefix = sl>0 and len(sp[-1])<=sl and (len(sp)==1 or len(sp[-2])==sl) and all(x.isdigit() for x in sp[-1])\n        return is_prefix and (not done or len(sp[-1])==0 or len(sp[-1])==sl)\n\n    @staticmethod\n    def is_valid_solution(guess):\n        return isinstance(guess, np.ndarray) and guess.ndim == 2 and all(0 < x <= 30 for x in guess.shape)\n\n    def max_new_tokens(self, safety_margin=1):\n        max_sized_reply = np.zeros([30, 30], dtype=int)\n        tokenized = self.tokenizer(self.fmt_reply([max_sized_reply]))['input_ids']\n        max_new_tokens = len(tokenized)\n        if tokenized[0]==self.tokenizer.bos_token_id: max_new_tokens -= 1\n        return max_new_tokens + safety_margin\n\n    def de_tokenize(self, tokens, scores=None):\n        import torch\n        tokens_cut = cut_at_token(tokens, self.tokenizer.eos_token_id)\n        de_tokenized = self.tokenizer.batch_decode([tokens_cut])[0]\n        score_val = None\n        if scores is not None:\n            tokens_with_eos = tokens[:len(tokens_cut)+1]\n            score_val = torch.nn.functional.log_softmax(torch.tensor(scores), dim=-1).numpy().copy()[np.arange(len(tokens_with_eos)), tokens_with_eos].sum()\n            number_token_ids = [self.tokenizer.vocab[k] for k in map(str, range(10))]\n            fault_token_id = self.collator_kwargs.get('fault_token_id')\n            if fault_token_id is not None: number_token_ids.append(fault_token_id)\n            number_token_ids = np.array(number_token_ids)\n            number_positions = (tokens_cut[..., np.newaxis] == number_token_ids).any(-1)\n            scores = scores[:len(tokens_cut), number_token_ids][number_positions]\n            scores = torch.nn.functional.log_softmax(torch.tensor(scores), dim=-1)[:, :10].numpy().copy()\n        return max(len(tokens)+1, len(tokens_cut)), score_val, de_tokenized, scores\n\n    def decode_to_array_single(self, text, score=None, limit_rows=30):\n        try:\n            by_rows = [row for row in [[int(x) for x in line if x.isdigit()] for line in text.split(self.dec_sep)] if len(row)]\n            if limit_rows and len(by_rows) > limit_rows:\n                by_rows = by_rows[:limit_rows]\n                limited = True\n            else: limited = False\n            decoded = np.array(by_rows, dtype=int)\n            if self.is_valid_solution(decoded):\n                try:\n                    assert score is not None\n                    decoded_flat = decoded.ravel()\n                    if limited: score = score[:len(decoded_flat)]\n                    score_all = score.reshape(decoded.shape + score.shape[1:])\n                    score_result = score[range(len(decoded_flat)), decoded_flat]\n                    score_reshaped = score_result.reshape(decoded.shape)\n                    score_cum_reshaped = score_result.cumsum().reshape(score_reshaped.shape)\n                    score_all_cum = score_cum_reshaped[..., np.newaxis] - score_reshaped[..., np.newaxis] + score_all\n                except: score_reshaped = score_cum_reshaped = np.full(decoded.shape, -float('inf'))\n                return {'output': decoded, 'score': score_reshaped, 'score_cum': score_cum_reshaped, 'score_all': score_all, 'score_all_cum': score_all_cum}\n        except: pass\n        return {}\n\n    def decode_to_array(self, text, score=None, limit_rows=30):\n        if not self.out2_use: text, score = [text], [score]\n        else:\n            text = text.split(self.out2_token)\n            if score is None: score = [None]*len(text)\n            else:\n                lengths = np.cumsum([len(list(filter(str.isdigit, t))) for t in text])\n                score = [score[s:e] for s, e in zip([0]+lengths[:-1].tolist(), lengths)]\n        return [self.decode_to_array_single(t, s) for t, s in zip(text, score)]\n\n    def get_corpus(self):\n        try:\n            old_min_wid, self.min_wid = self.min_wid, min(self.min_wid, 2)\n            return self.fmt_train([{'input': [[i] for i in range(10)], 'output': [[i] for i in range(10)]}]*3, last_is_challenge=True, pretext_split=True)\n        finally: self.min_wid = old_min_wid\n\n    def get_data_collator(self):\n        if not self.masking: return None\n        from transformers import DataCollatorForLanguageModeling\n        collator_params = dict(tokenizer=self.tokenizer, mlm=False)\n        pass_out2_token = self.tokenizer.vocab[self.out2_token] if self.out2_use and self.masking==1 else None\n        if self.masking:\n            assert not self.collator_kwargs.get('mask_first_output') or self.masking==1\n            data_collator = get_class_MyDataCollator()(\n                **collator_params,\n                instruction_template=[self.inp_prefix, self.tokenizer.bos_token][self.masking - 1],\n                response_template=[self.out_prefix, (self.out2_token if self.out2_use else self.rpl_sep)][self.masking - 1],\n            ).setup(out2_token_id=pass_out2_token, **self.collator_kwargs)\n        else:\n            assert not self.collator_kwargs, 'only supported with masking on'\n            data_collator = DataCollatorForLanguageModeling(**collator_params)\n        return data_collator\n\n    def get_output_token_ids(self):\n        assert not self.out2_use\n        num_tokens = [self.tokenizer.vocab[str(i)] for i in range(10)]\n        sep_tokens = [tok for txt in [self.arr_beg, self.arr_sep, self.arr_end, self.exa_sep] if txt for tok in self.tokenizer(txt)['input_ids'][1:]]\n        sep_tokens.append(self.tokenizer.eos_token_id)\n        return num_tokens + sorted(set(sep_tokens))\n\nArcFormatter_pretext2 = lambda **kwargs: ArcFormatter(masking=1, inp_prefix='I', out_prefix='O', arr_sep='\\n', arr_end='\\n', pretext='ABCDEFGHJKLMNPQRSTUVWXYZ', pretext_corpus_split='\\n', **kwargs)\nArcFormatter_pretext3 = lambda **kwargs: ArcFormatter(masking=1, inp_prefix='I', out_prefix='O', arr_sep='\\n', arr_end='\\n', pretext='ABCDEFGHJKLMNPQRSTUVWXYZabcdefghjklmnpqrstuvwxyz', pretext_corpus_split='\\n', **kwargs)\nArcFormatter_premix_2 = lambda **kwargs: ArcFormatter(masking=1, inp_prefix='I', out_prefix='O', arr_sep='\\n', arr_end='\\n', pretext='ABCDEFGHJKLMNPQRSTUVWXYZ', pre_out=['+/-=']*99, pretext_corpus_split='\\n', **kwargs)\nArcFormatter_premix_3 = lambda **kwargs: ArcFormatter(masking=1, inp_prefix='I', out_prefix='O', arr_sep='\\n', arr_end='\\n', pretext='ABCDEFGHJKLMNPQRSTUVWXYZabcdefghjklmnpqrstuvwxyz', pre_out=['+/-=']*99, pretext_corpus_split='\\n', **kwargs)\n\navailable_formatters = dict(\n    ArcFormatter_pretext2=ArcFormatter_pretext2,\n    ArcFormatter_pretext3=ArcFormatter_pretext3,\n    ArcFormatter_premix_2=ArcFormatter_premix_2,\n    ArcFormatter_premix_3=ArcFormatter_premix_3,\n)","metadata":{"execution":{"iopub.execute_input":"2025-04-04T13:51:17.65119Z","iopub.status.busy":"2025-04-04T13:51:17.650892Z","iopub.status.idle":"2025-04-04T13:51:17.669414Z","shell.execute_reply":"2025-04-04T13:51:17.668865Z"},"papermill":{"duration":0.027922,"end_time":"2025-04-04T13:51:17.670792","exception":false,"start_time":"2025-04-04T13:51:17.64287","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Selection Algorithms for ARC Competition Solutions\n\nThe code defines a collection of selection algorithms designed to choose optimal solutions from multiple model predictions for the Abstraction and Reasoning Corpus (ARC) competition. This is a critical component in the submission pipeline, as it determines which predictions will be submitted as final answers.\n\nAt its core, the selection module offers several strategies for filtering and ranking candidate solutions based on different criteria. The simplest approach is `first_only`, which simply takes the first prediction, reflecting a high confidence in the model's initial guess. The `keep_order` algorithm preserves all predictions in their original sequence, useful when the ordering already reflects confidence levels. For eliminating redundancy, `keep_order_unique` builds on this by removing duplicate solutions.\n\nThe more sophisticated selection strategies leverage scoring mechanisms. `get_best_shape_by_score` groups predictions by their output shape (dimensions) and identifies the most promising shape based on a scoring function. This is particularly valuable in ARC problems where correct solutions often share consistent dimensions. The `score_sum` function extends this concept by accumulating scores for unique outputs while optionally preferring answers that match the most common output shape.\n\nTwo notable scoring implementations are provided: `score_all_probsum`, which converts log probabilities to probabilities and sums them to rank solutions, and `score_full_probmul_3`, which incorporates both inference scores and augmented scores with a baseline offset of 3. This combined approach aims to balance the model's direct confidence (inference scores) with additional evaluation metrics (augmented scores) for more robust selection.\n\nThe code includes utility functions like `hashable` and `make_unique` to handle the array-based outputs, ensuring proper comparison and deduplication. All these algorithms are collected in `selection_algorithms`, allowing for benchmarking different strategies against each other to determine the optimal approach for the final submission.","metadata":{"papermill":{"duration":0.00668,"end_time":"2025-04-04T13:51:17.684278","exception":false,"start_time":"2025-04-04T13:51:17.677598","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%%writefile selection.py\nimport numpy as np\n\ndef hashable(guess):\n    return tuple(map(tuple, guess))\n\ndef make_unique(guess_list, indices=None):\n    used = set()\n    out = []\n    out_ind = []\n    for i, g in enumerate(guess_list):\n        h = hashable(g)\n        if h not in used:\n            used.add(h)\n            out.append(np.array(g))\n            if indices is not None: out_ind.append(indices[i])\n    return out if indices is None else (out, out_ind)\n\ndef first_only(guesses):\n    return [g['output'] for g in guesses.values()][:1]\n\ndef keep_order(guesses):\n    return [g['output'] for g in guesses.values()]\n\ndef keep_order_unique(guesses):\n    return make_unique(keep_order(guesses))\n\ndef get_best_shape_by_score(guess_list, getter, once_per_result=True):\n    seen_outputs = set()\n    shape_scores = {}\n    for i, g in enumerate(guess_list):\n        shape = tuple(g['output'].shape)\n        scores = shape_scores[shape] = shape_scores.get(shape, [[], []])\n        scores[1].append(i)\n        h = hashable(g['output'])\n        if h in seen_outputs: continue\n        if once_per_result: seen_outputs.add(h)\n        scores[0].append(g)\n    shape_scores = [(getter(scores), shape, indices) for shape, (scores, indices) in shape_scores.items()]\n    shape_scores = sorted(shape_scores, key=(lambda x: x[0]), reverse=True)\n    return shape_scores[0]\n\ndef score_sum(guesses, getter, shape_getter=None, prefer_common_shape=True):\n    if shape_getter is None: shape_getter = getter\n    guess_list = list(guesses.values())\n    common_shape_indices = set(get_best_shape_by_score(guess_list, shape_getter)[2]) if prefer_common_shape else []\n    scores = {}\n    for i, g in enumerate(guess_list):\n        h = hashable(g['output'])\n        x = scores[h] = scores.get(h, [i in common_shape_indices, [], g['output']])\n        x[1].append(g)\n    scores = [(cs, getter(sc), o) for cs, sc, o in scores.values()]\n    scores = sorted(scores, key=(lambda x: x[:2]), reverse=True)\n    ordered_outputs = [x[-1] for x in scores]\n    return ordered_outputs\n\ngetter_all_probsum = lambda guesses: sum(np.exp(g['score_val']) for g in guesses)\ndef score_all_probsum(guesses): return score_sum(guesses, getter_all_probsum)\n\ndef getter_full_probmul(p):\n    def _getter(guesses, baseline=p):\n        inf_score = sum([g['score_val']+baseline for g in guesses])\n        aug_score = np.mean([sum(s+baseline for s in g['score_multi_nl']) for g in guesses])\n        return inf_score + aug_score\n    return _getter\n\ndef score_full_probmul_3(guesses): return score_sum(guesses, getter_full_probmul(3), prefer_common_shape=False)\n\nselection_algorithms = [\n    first_only,\n    keep_order,\n    keep_order_unique,\n    score_all_probsum,\n    score_full_probmul_3,\n]","metadata":{"execution":{"iopub.execute_input":"2025-04-04T13:51:17.699117Z","iopub.status.busy":"2025-04-04T13:51:17.698563Z","iopub.status.idle":"2025-04-04T13:51:17.703506Z","shell.execute_reply":"2025-04-04T13:51:17.702952Z"},"papermill":{"duration":0.013721,"end_time":"2025-04-04T13:51:17.704877","exception":false,"start_time":"2025-04-04T13:51:17.691156","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Asynchronous Subprocess Handling with Streaming Output\n\nThis code provides a set of asynchronous utilities for executing and monitoring subprocesses in Python. Built on top of Python's `asyncio` library, these functions enable efficient parallel execution of external processes while capturing their output streams in real-time.\n\nThe `stream_reader` function serves as the core component, continuously reading from a subprocess's output stream (either stdout or stderr) in manageable chunks of 4KB. It implements a clever buffering mechanism to ensure complete lines are processed properly. By appending a sentinel character ('X') and using Python's unpacking syntax, it elegantly separates complete lines from partial data that might be cut off mid-line. Each complete line is optionally prefixed with an identifier and directed to the specified output stream.\n\nThe `wait_for_subprocess` function builds on this foundation by simultaneously monitoring both the stdout and stderr streams of a single subprocess. It uses `asyncio.gather` to concurrently process both streams until completion, then waits for the subprocess to terminate and returns its exit code. The `print_output` parameter provides control over whether the subprocess output should be displayed, while the `id` parameter helps distinguish between outputs from different processes when multiple are running.\n\nFinally, `wait_for_subprocesses` extends this capability to handle multiple subprocesses concurrently. It automatically assigns sequential numeric identifiers to each process when more than one is being monitored, making it easier to distinguish their outputs in a multiplexed console display.\n\nThis asynchronous approach is particularly valuable in data processing pipelines that might involve multiple external tools or long-running computations. Rather than blocking while waiting for each process to complete sequentially, these functions allow the Python program to efficiently manage multiple concurrent tasks, potentially improving overall throughput while maintaining organized output capture.","metadata":{"papermill":{"duration":0.006815,"end_time":"2025-04-04T13:51:17.718539","exception":false,"start_time":"2025-04-04T13:51:17.711724","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%%writefile async_tools.py\nimport sys\nimport asyncio\n\nasync def stream_reader(stream, id, to):\n    id = '' if id is None else f'{id}. '\n    data = b''\n    while True:\n        read = await stream.read(n=4096)\n        if not read: break\n        if to is not None:\n            *complete_lines, data = (data + read + b'X').splitlines()\n            data = data[:-1]\n            for line in complete_lines:\n                line = line.rstrip()\n                if line: print(f\"{id}{line.decode('utf-8')}\", file=to, end='\\n', flush=True)\n\nasync def wait_for_subprocess(subprocess, print_output=False, id=None):\n    await asyncio.gather(\n            stream_reader(subprocess.stdout, id, (sys.stdout if print_output else None)),\n            stream_reader(subprocess.stderr, id, (sys.stderr if print_output else None)),\n        )\n    return await subprocess.wait()\n\nasync def wait_for_subprocesses(*processes, print_output=False):\n    return await asyncio.gather(*[wait_for_subprocess(p, print_output=print_output, id=i if len(processes)>1 else None) for i, p in enumerate(processes)])","metadata":{"execution":{"iopub.execute_input":"2025-04-04T13:51:17.733395Z","iopub.status.busy":"2025-04-04T13:51:17.73288Z","iopub.status.idle":"2025-04-04T13:51:17.73698Z","shell.execute_reply":"2025-04-04T13:51:17.736423Z"},"papermill":{"duration":0.012944,"end_time":"2025-04-04T13:51:17.738367","exception":false,"start_time":"2025-04-04T13:51:17.725423","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Comprehensive Framework for ARC Challenge Solution Pipeline\n\nThis code defines a robust configuration and execution framework for tackling the Abstraction and Reasoning Corpus (ARC) challenge. It serves as the central orchestration module for the entire solution pipeline, connecting data loading, model training, inference, and result submission generation.\n\nThe file begins by establishing essential paths and configuration settings, including locations for the ARC challenge data and temporary storage directories for model weights and inference outputs. It then loads the test dataset and conditionally loads solution data if working with a \"fake\" test set (likely for validation purposes). The configuration specifies the use of a pre-trained NeMo Mini model with the specialized ArcFormatter_premix_3 for data processing.\n\nAt the heart of the implementation are two key preparation functions: `prepare_run` and `prepare_dataset`. The `prepare_run` function configures the model environment, including GPU assignment and model initialization with LoRA (Low-Rank Adaptation) fine-tuning parameters. It uses Unsloth's 4-bit quantization for efficient memory usage, applies parameter-efficient fine-tuning to various model components with carefully chosen hyperparameters (including rank-stabilized LoRA), and optimizes for long contexts with gradient checkpointing.\n\nThe `prepare_dataset` function handles dataset preparation with sophisticated pre-processing techniques. For multi-GPU training, it supports both random splitting and length-based distribution of tasks. The function applies different augmentation strategies depending on whether the dataset is being prepared for training or inference. Training data undergoes rotation, transformation, permutation, and sequence shuffling, while ensuring proper length constraints. Inference data is sorted by input length, augmented, and interleaved to optimize processing.\n\nThe pipeline continues with two execution functions: `start_training` and `start_inference`. The training function fine-tunes the model using a parameter-efficient approach with 8-bit optimization, cosine learning rate scheduling, and carefully tuned hyperparameters. The inference function applies the trained model to generate solutions, with optional task-specific fine-tuning through the Retrainer class and augmented scoring to enhance solution quality.\n\nA notable protective mechanism is the `RemapCudaOOM` context manager, which gracefully handles CUDA out-of-memory errors by creating a placeholder submission file rather than failing catastrophically. This ensures that even under resource constraints, the system can produce a valid competition entry.\n\nThis comprehensive framework represents a sophisticated approach to the ARC challenge, incorporating advanced techniques in efficient model fine-tuning, data augmentation, and robust error handling to maximize performance within Kaggle's computational constraints.","metadata":{"papermill":{"duration":0.006868,"end_time":"2025-04-04T13:51:17.752175","exception":false,"start_time":"2025-04-04T13:51:17.745307","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%%writefile common_stuff.py\n# common configuration for training and evaluation\nfrom arc_loader import *\nfrom model_runner import *\nfrom selection import *\nfrom async_tools import *\nimport time\nimport random\nimport numpy as np\nimport torch\n\n# \nGLOBAL_SEED = 42\n\n# \ndef set_all_seeds(seed=GLOBAL_SEED):\n    \"\"\"\"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    # CUDA\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Python\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\n# \nset_all_seeds()\n\n# paths\ntmp_dir = '/kaggle/temp'\narc_challenge_file = '/kaggle/input/arc-prize-2025/arc-agi_test_challenges.json'\narc_solutions_file = '/kaggle/input/arc-prize-2025/arc-agi_training_solutions.json'\nmodel_temp_storage = os.path.join(tmp_dir, 'finetuned_model')\ninfer_temp_storage = os.path.join(tmp_dir, 'inference_outputs')\nscore_temp_storage = os.path.join(tmp_dir, 'inference_scoring')\n\n# load datasets\narc_test_set = ArcDataset.from_file(arc_challenge_file)\nif arc_test_set.is_fake: arc_test_set.load_replies(arc_solutions_file)\n#arc_test_set.is_fake = False  # force full run\n#arc_train_set = ArcDataset.from_file('/kaggle/input/arc-prize-2024/arc-agi_training_challenges.json')\n\n# models\nbase_model, MyFormatter, perm_aug, max_seq_length_train, mask_first = '/kaggle/input/wb55l_nemomini_fulleval/transformers/default/1', ArcFormatter_premix_3, 'rnd_all', 4224, 0\n\n# training & inference\ntrain_epochs = 4\nmulti_gpu_train = True\nmulti_gpu_random_split = True\nmax_seq_length_infer = 8192\nprime_on_single_task = False\ninfer_params = dict(min_prob=0.17, store=infer_temp_storage, use_turbo=True)\n\n# scoring\nuse_aug_score = True\naug_score_params = dict(tp=True, rot=True, perm=perm_aug, shfl_ex=True, make_unique=True, max_len=max_seq_length_infer)\nsubmission_select_algo = score_full_probmul_3 if use_aug_score else score_all_probsum\n\ndef prepare_run(model_path, load_lora=None, train=False, gpu=None, **kwargs):\n\n    # GPU\n    seed = GLOBAL_SEED + (0 if gpu is None else gpu)\n    set_all_seeds(seed)\n    \n    if gpu is not None:\n        os.environ[\"CUDA_DEVICE_ORDER\"   ] = \"PCI_BUS_ID\"\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu)\n\n    model, tokenizer, formatter = prepare_model(  # base model configuration\n        model=model_path,\n        local_files_only=True,\n        mode='unsloth_4bit',\n        #shrink_embedding=8000,\n        max_seq_length=max_seq_length_train,\n        formatter=MyFormatter,\n        peft=([dict(\n            r=64,  # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n            target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj', 'embed_tokens', 'lm_head'],\n            lora_alpha=16,\n            lora_dropout=0,  # Supports any, but = 0 is optimized\n            bias=\"none\",  # Supports any, but = \"none\" is optimized\n            use_gradient_checkpointing=True,  # True or \"unsloth\" for very long context\n            random_state=42,\n            use_rslora=True,  # We support rank stabilized LoRA\n            loftq_config=None,  # And LoftQ\n        )] if train or load_lora else []) + ([load_lora] if load_lora else []),\n        **kwargs\n    )\n    \n    if train and mask_first: formatter.collator_kwargs.update(mask_first_output=mask_first)\n\n    return model, formatter\n\ndef prepare_dataset(formatter, train, gpu=None):\n\n    # \n    seed = GLOBAL_SEED + (0 if gpu is None else gpu)\n    set_all_seeds(seed)\n\n    ds = arc_test_set\n    if multi_gpu_train and gpu is not None:\n        if multi_gpu_random_split:\n            ds = ds.shuffled(seed=123)\n            # Split into 4 parts instead of 2\n            quarter_size = len(ds.keys) // 4\n            if gpu < 3:\n                ds = ds.split_at_pos(quarter_size)[0]  # First quarter for GPU 0\n                if gpu > 0:\n                    ds = ds.split_at_pos(quarter_size * gpu // (gpu + 1))[1]  # Subsequent quarters\n            else:\n                ds = ds.split_at_pos(3 * quarter_size)[1]  # Last quarter for GPU 3\n        else:\n            ds = ds.sorted_by_len(formatter=formatter, name='input', max_of_transposed=True)\n            # 4-GPU rotation pattern instead of 2-GPU\n            assignment = ([0,1,2,3]*ds.length())[:ds.length()][::-1]\n            ds = ds.change_keys((np.array(ds.keys)[np.array(assignment)==gpu]).tolist())\n    \n    # Rest of the function remains the same\n    if train:\n        ds = ds.remove_replies()\n        ds = ds.augment(tp=True, rot=True, perm=perm_aug, n=(2 if arc_test_set.is_fake else train_epochs), shfl_ex=True, shfl_keys=True)\n        ds = ds.cut_to_len(formatter=formatter, name='text', max_len=max_seq_length_train, max_new_tokens=0)\n        if arc_test_set.is_fake: ds = ds.sorted_by_len(formatter=formatter, name='text', reverse=True)\n    else:\n        ds = ds.sorted_by_len(formatter=formatter, name='input', max_of_transposed=True)\n        ds = ds.split_multi_replies()\n        ds = ds.augment(tp=True, rot=True, n=2, seed=42, perm=perm_aug, shfl_ex=True).interleave(ds.length())\n        ds = ds.cut_to_len(formatter=formatter, name='input', max_len=max_seq_length_infer)\n        if arc_test_set.is_fake: ds.keys = ds.keys[:128]\n    return ds\n\ndef start_training(gpu):\n    # GPU\n    seed = GLOBAL_SEED + gpu\n    set_all_seeds(seed)\n\n    try:\n        storage_path = f'{model_temp_storage}_gpu{gpu}'\n        if (gpu==0 or multi_gpu_train) and not os.path.exists(storage_path):\n            with RemapCudaOOM():\n                model, formatter = prepare_run(base_model, train=True, gpu=gpu)\n                dataset = prepare_dataset(formatter, train=True, gpu=gpu if multi_gpu_train else None)\n                model, trainer_stats = training_run(\n                    model, formatter, dataset, store=storage_path,\n                    max_seq_length=max_seq_length_train,\n                    grad_acc_fix=False,\n                    train_args=dict(\n                        per_device_train_batch_size=8, #x4\n                        gradient_accumulation_steps=1, #half\n                        warmup_steps=100, \n                        num_train_epochs=1,\n                        max_steps=5 if arc_test_set.is_fake else -1,\n                        #max_steps=10 if arc_test_set.is_fake else 20,\n                        learning_rate=2e-4, #double\n                        embedding_learning_rate=1e-5, \n                        logging_steps=10,\n                        optim=\"adamw_8bit\",\n                        weight_decay=0.01,  # 0.01,\n                        lr_scheduler_type='cosine',  # \"linear\", \"cosine\",\n                        seed=42,\n                        output_dir=os.path.join(tmp_dir, 'checkpoints'),\n                        save_strategy=\"no\",\n                        report_to='none',\n                    ),\n                )\n                mem_info()\n    finally: os.makedirs(f'{storage_path}_done', exist_ok=True)\n\ndef start_inference(gpu):\n    # GPU\n    seed = GLOBAL_SEED + gpu + 100  # \n    set_all_seeds(seed)\n    \n    storage_path = f'{model_temp_storage}_gpu{gpu if multi_gpu_train else 0}'\n    while not os.path.exists(f'{storage_path}_done'): time.sleep(15)\n    with RemapCudaOOM():\n        model, formatter = prepare_run(storage_path, gpu=gpu)\n        dataset = prepare_dataset(formatter, train=False, gpu=gpu)\n        retrainer = None if not prime_on_single_task else Retrainer(\n            n=32,\n            aug_opts=dict(perm=perm_aug, shfl_ex=True),\n            reload_state_dict=get_and_fix_peft_weights(storage_path),\n            formatter=formatter,\n            max_seq_length=max_seq_length_infer,\n            grad_acc_fix=False,\n            train_args=dict(\n                per_device_train_batch_size=8, #x4\n                gradient_accumulation_steps=1, #half\n                warmup_steps=8,\n                num_train_epochs=2,\n                learning_rate=5e-5,\n                embedding_learning_rate=0,\n                logging_steps=8,\n                optim=\"adamw_8bit\",\n                weight_decay=0.00,  # 0.01,\n                lr_scheduler_type='constant',  # \"linear\", \"cosine\",\n                seed=42,\n                output_dir='tmp_output',\n                save_strategy='no',\n                report_to='none',\n            ),\n        )\n        decoder = Decoder(formatter, arc_test_set.split_multi_replies(), n_guesses=2, prob_baseline=0.05)\n        inference_run_v2(model, formatter, dataset, decoder, retrain=retrainer, **infer_params)\n        if use_aug_score or arc_test_set.is_fake: decoder.calc_augmented_scores(model=model, store=score_temp_storage, **aug_score_params)\n        mem_info()\n\nclass RemapCudaOOM:\n    def __enter__(self): pass\n    def __exit__(self, exc_type, exc_value, traceback):\n        oom_errors = [\"CUDA out of memory\", \"Make sure you have enough GPU RAM\", \"does not fit any GPU's remaining memory\"]\n        if exc_value and any(x in str(exc_value) for x in oom_errors):\n            with open('submission.json', 'w') as f: f.write('cause submission scoring error')\n","metadata":{"execution":{"iopub.execute_input":"2025-04-04T13:51:17.7671Z","iopub.status.busy":"2025-04-04T13:51:17.766861Z","iopub.status.idle":"2025-04-04T13:51:17.774942Z","shell.execute_reply":"2025-04-04T13:51:17.77439Z"},"papermill":{"duration":0.017248,"end_time":"2025-04-04T13:51:17.776316","exception":false,"start_time":"2025-04-04T13:51:17.759068","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Environment Setup for ARC Challenge Pipeline\n\nThis code snippet represents the initialization phase of the ARC challenge solution pipeline. It performs critical setup tasks before the main training and inference processes begin. Let me walk through what's happening:\n\n## Environment Configuration and Unsloth Installation\n\nThe code begins by importing all components from the `common_stuff` module, which contains the core functionality for the ARC solution pipeline as seen in previous sections. It then disables Weights & Biases logging by setting the `WANDB_DISABLED` environment variable to `\"true\"`.\n\nA key component is the custom installation and patching of the Unsloth library. Unsloth provides efficient optimization techniques for large language models, but requires specific modifications to work properly in this environment:\n\n1. The code checks if Unsloth is already installed by looking for a marker file\n2. If not installed, it:\n   - Uninstalls existing PyTorch and Accelerate packages to avoid conflicts\n   - Installs Unsloth from a local wheel file (avoiding internet downloads, which is important in Kaggle environments)\n   - Applies several patches to the Unsloth source code:\n     - Disables the `get_statistics()` function to fix a delay bug\n     - Removes multi-GPU detection restrictions to enable distributed training\n   - Creates a marker file to indicate successful installation\n\nThis custom installation approach ensures compatibility with the Kaggle environment while enabling optimized training performance.\n\n## Training Preparation and Cleanup\n\nAfter setting up the environment, the code prepares for training by:\n\n1. Removing any \"done\" signal files from previous runs for both GPUs (0 and 1)\n   - This ensures that the training and inference processes won't mistakenly think a previous run completed successfully\n\n2. For debugging scenarios (when using a \"fake\" test set), there are commented-out commands to remove previous model outputs and temporary files\n   - These cleanup commands are disabled (commented out) but could be enabled for debugging purposes\n\nThis initialization routine establishes a clean, optimized environment for the subsequent model training and inference phases of the ARC challenge solution pipeline. The modified Unsloth library will enable efficient fine-tuning of the large language model with quantization and other optimizations tailored to this specific task.","metadata":{"papermill":{"duration":0.007161,"end_time":"2025-04-04T13:51:17.790477","exception":false,"start_time":"2025-04-04T13:51:17.783316","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from common_stuff import *\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\nif not os.path.exists(os.path.join(tmp_dir, 'unsloth_installed')):  # unsloth offline install - https://stackoverflow.com/a/51646354\n    !pip uninstall --yes torch accelerate\n    !pip install --no-index --find-links=/kaggle/input/unsloth-2024-9-post4/wheelhouse unsloth\n    #!pip uninstall --yes accelerate fastai torch torchaudio transformers\n    #!pip install --no-index --find-links=/kaggle/input/unsloth-2024-10-7/wheelhouse unsloth  # do not use grad_acc_fix - trains very slow\n    #!sed -i 's/if ((post_check - pre_check) >= 1).sum() > 1:/if False:/g' /opt/conda/lib/python3.10/site-packages/unsloth/models/llama.py\n    # fix delay bug in get_statistics()\n    !sed -i 's/^def get_statistics():/def get_statistics():\\n if False:/g' /opt/conda/lib/python3.10/site-packages/unsloth/models/_utils.py\n    # fix faulty unsloth multi-gpu detection\n    !sed -i \"s/raise RuntimeError('Unsloth currently does not support multi GPU setups - but we are working on it!')/pass/g\" /opt/conda/lib/python3.10/site-packages/unsloth/tokenizer_utils.py /opt/conda/lib/python3.10/site-packages/unsloth/models/llama.py /opt/conda/lib/python3.10/site-packages/unsloth/models/vision.py\n    os.makedirs(os.path.join(tmp_dir, 'unsloth_installed'), exist_ok=True)\n    print('Unsloth installed & patched.')\n\nfor gpu in [0, 1]: \n    signal_path = f'{model_temp_storage}_gpu{gpu}_done'\n    if os.path.exists(signal_path): os.rmdir(signal_path)\n\nif arc_test_set.is_fake:  # cleanup? (for debugging)\n    #!rm -R /kaggle/temp/finetuned_model*\n    #!rm -R /kaggle/temp/inference_outputs\n    #!rm -R /kaggle/temp/inference_scoring\n    #!ls /kaggle/temp\n    pass\n","metadata":{"execution":{"iopub.execute_input":"2025-04-04T13:51:17.805344Z","iopub.status.busy":"2025-04-04T13:51:17.804983Z","iopub.status.idle":"2025-04-04T13:53:13.74439Z","shell.execute_reply":"2025-04-04T13:53:13.743559Z"},"papermill":{"duration":115.948666,"end_time":"2025-04-04T13:53:13.746093","exception":false,"start_time":"2025-04-04T13:51:17.797427","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Asynchronous Training Process Initialization\n\nThis code cell initiates a background training process for the ARC challenge solution pipeline. Let me explain the critical aspects of what's happening here:\n\nThe cell uses a special Jupyter cell magic `%%python --bg --proc train_proc0` which instructs the notebook to run the Python code in a separate background process rather than within the main notebook execution thread. The `--proc train_proc0` parameter assigns a specific name to this process, making it identifiable for monitoring and management purposes.\n\nWithin this background process, the code imports all components from the `common_stuff` module, which contains the comprehensive framework for model configuration, dataset preparation, and training execution that we examined earlier. This module provides access to the model architecture, training parameters, data augmentation strategies, and other essential components of the solution pipeline.\n\nThe core action is the call to `start_training(gpu=0)`, which initiates the model training process on GPU 0. As detailed in the `common_stuff` module, this function will:\n\n1. Establish a unique storage path for this GPU's model weights\n2. Configure the base model with LoRA fine-tuning parameters\n3. Prepare the dataset with appropriate augmentations for training\n4. Execute the training process with carefully tuned hyperparameters\n5. Create a signal file upon completion to indicate that training has finished\n\nRunning this process in the background allows the notebook to remain responsive while the computationally intensive training occurs. This approach is particularly valuable in a multi-GPU setup, as it enables the initiation of parallel training processes across available GPUs, potentially accelerating the overall solution development. The named process also facilitates monitoring or termination if needed during the potentially lengthy training process.\n\nThis background execution model is a sophisticated approach to managing computational resources in Jupyter environments, especially for the resource-intensive tasks involved in state-of-the-art AI competition solutions.","metadata":{"papermill":{"duration":0.009428,"end_time":"2025-04-04T13:53:13.765013","exception":false,"start_time":"2025-04-04T13:53:13.755585","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Simplified ARC data visualization script (English version)\nfrom arc_loader import *\nimport matplotlib.pyplot as plt\nfrom matplotlib import colors\nimport numpy as np\nimport json\nimport os\n\n# Create ARC color map\ncmap = colors.ListedColormap(\n    ['#000000', '#0074D9', '#FF4136', '#2ECC40', '#FFDC00',\n     '#AAAAAA', '#F012BE', '#FF851B', '#7FDBFF', '#870C25'])\nnorm = colors.Normalize(vmin=0, vmax=9)\n\n# Load data directly from file\narc_challenge_file = '/kaggle/input/arc-prize-2025/arc-agi_test_challenges.json'\n\n# Load original data\nwith open(arc_challenge_file, 'r') as f:\n    arc_data = json.load(f)\n\n# Set random seeds\nnp.random.seed(42)\nrandom.seed(42)\n\ndef visualize_arc_example(train_data, test_data, task_id):\n    \"\"\"Visualize training and test data for an ARC task\"\"\"\n    # Get number of training and test examples\n    n_train = len(train_data)\n    n_test = len(test_data)\n    \n    # Create figure large enough for all examples\n    fig, axes = plt.subplots(2, max(n_train, n_test), figsize=(4*max(n_train, n_test), 8))\n    fig.suptitle(f\"Task ID: {task_id}\", fontsize=16)\n    \n    # Visualize training data\n    for i in range(n_train):\n        # Input\n        axes[0, i].imshow(train_data[i]['input'], cmap=cmap, norm=norm)\n        axes[0, i].grid(True, which='both', color='lightgrey', linewidth=0.5)\n        axes[0, i].set_title(f\"Training #{i+1} - Input\")\n        axes[0, i].set_xticks([])\n        axes[0, i].set_yticks([])\n        \n        # Output\n        axes[1, i].imshow(train_data[i]['output'], cmap=cmap, norm=norm)\n        axes[1, i].grid(True, which='both', color='lightgrey', linewidth=0.5)\n        axes[1, i].set_title(f\"Training #{i+1} - Output\")\n        axes[1, i].set_xticks([])\n        axes[1, i].set_yticks([])\n    \n    # Handle test data visualization\n    for i in range(n_test):\n        if i < n_train:\n            # Already have training data in this column\n            pass\n        else:\n            # Hide unused training cells\n            if i >= n_train:\n                axes[0, i].axis('off')\n                axes[1, i].axis('off')\n    \n    # Show first test input\n    if n_test > 0:\n        # Create separate figure for test input\n        plt.figure(figsize=(5, 5))\n        plt.imshow(test_data[0]['input'], cmap=cmap, norm=norm)\n        plt.grid(True, which='both', color='lightgrey', linewidth=0.5)\n        plt.title(f\"Test Input - {task_id}\")\n        plt.xticks([])\n        plt.yticks([])\n        plt.show()\n        \n    plt.tight_layout()\n    plt.subplots_adjust(top=0.9)\n    plt.show()\n\n# Simulate 4 GPU data splitting\ntask_ids = list(arc_data.keys())\nrandom.shuffle(task_ids)  # Shuffle task order\n\n# Assign tasks to each GPU\ngpu_tasks = {}\nfor gpu_id in range(4):\n    # Simple equal division - each GPU gets 1/4 of tasks\n    start_idx = gpu_id * len(task_ids) // 4\n    end_idx = (gpu_id + 1) * len(task_ids) // 4\n    gpu_tasks[gpu_id] = task_ids[start_idx:end_idx]\n\n# Display training data samples for each GPU\nfor gpu_id in range(4):\n    assigned_tasks = gpu_tasks[gpu_id]\n    print(f\"\\n{'='*40}\\nGPU {gpu_id} Training Data Samples\\n{'='*40}\")\n    print(f\"GPU {gpu_id} assigned {len(assigned_tasks)} training tasks\")\n    \n    # Show only first 3 examples\n    samples = assigned_tasks[:3]\n    \n    for task_id in samples:\n        print(f\"\\nTask: {task_id}\")\n        \n        # Get training and test data for this task\n        train_data = arc_data[task_id]['train']\n        test_data = arc_data[task_id]['test']\n        \n        # Visualize\n        visualize_arc_example(train_data, test_data, task_id)\n        \n        # Print data matrices\n        print(\"Training Input (first example):\")\n        print(np.array(train_data[0]['input']))\n        print(\"\\nTraining Output (first example):\")\n        print(np.array(train_data[0]['output']))\n        print(\"-\" * 40)","metadata":{"execution":{"iopub.execute_input":"2025-04-04T13:53:13.785207Z","iopub.status.busy":"2025-04-04T13:53:13.784296Z","iopub.status.idle":"2025-04-04T13:53:19.05876Z","shell.execute_reply":"2025-04-04T13:53:19.058084Z"},"papermill":{"duration":5.286141,"end_time":"2025-04-04T13:53:19.060268","exception":false,"start_time":"2025-04-04T13:53:13.774127","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%python --bg --proc train_proc0\nfrom common_stuff import *\nstart_training(gpu=0)","metadata":{"execution":{"iopub.execute_input":"2025-04-04T13:53:19.098136Z","iopub.status.busy":"2025-04-04T13:53:19.097855Z","iopub.status.idle":"2025-04-04T13:53:19.120058Z","shell.execute_reply":"2025-04-04T13:53:19.11946Z"},"papermill":{"duration":0.042624,"end_time":"2025-04-04T13:53:19.121504","exception":false,"start_time":"2025-04-04T13:53:19.07888","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%python --bg --proc train_proc1\nfrom common_stuff import *\nstart_training(gpu=1)","metadata":{"execution":{"iopub.execute_input":"2025-04-04T13:53:19.158122Z","iopub.status.busy":"2025-04-04T13:53:19.157863Z","iopub.status.idle":"2025-04-04T13:53:19.162759Z","shell.execute_reply":"2025-04-04T13:53:19.162208Z"},"papermill":{"duration":0.024793,"end_time":"2025-04-04T13:53:19.164183","exception":false,"start_time":"2025-04-04T13:53:19.13939","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%python --bg --proc train_proc2\nfrom common_stuff import *\nstart_training(gpu=2)","metadata":{"execution":{"iopub.execute_input":"2025-04-04T13:53:19.201234Z","iopub.status.busy":"2025-04-04T13:53:19.200642Z","iopub.status.idle":"2025-04-04T13:53:19.206623Z","shell.execute_reply":"2025-04-04T13:53:19.205445Z"},"papermill":{"duration":0.02736,"end_time":"2025-04-04T13:53:19.209443","exception":false,"start_time":"2025-04-04T13:53:19.182083","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%python --bg --proc train_proc3\nfrom common_stuff import *\nstart_training(gpu=3)","metadata":{"execution":{"iopub.execute_input":"2025-04-04T13:53:19.32093Z","iopub.status.busy":"2025-04-04T13:53:19.320334Z","iopub.status.idle":"2025-04-04T13:53:19.343524Z","shell.execute_reply":"2025-04-04T13:53:19.342374Z"},"papermill":{"duration":0.097596,"end_time":"2025-04-04T13:53:19.347782","exception":false,"start_time":"2025-04-04T13:53:19.250186","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%python --bg --proc infer_proc0\nfrom common_stuff import *\nstart_inference(gpu=0)","metadata":{"execution":{"iopub.execute_input":"2025-04-04T13:53:19.406042Z","iopub.status.busy":"2025-04-04T13:53:19.405477Z","iopub.status.idle":"2025-04-04T13:53:19.415586Z","shell.execute_reply":"2025-04-04T13:53:19.41447Z"},"papermill":{"duration":0.032516,"end_time":"2025-04-04T13:53:19.418262","exception":false,"start_time":"2025-04-04T13:53:19.385746","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%python --bg --proc infer_proc1\nfrom common_stuff import *\nstart_inference(gpu=1)","metadata":{"execution":{"iopub.execute_input":"2025-04-04T13:53:19.551717Z","iopub.status.busy":"2025-04-04T13:53:19.551247Z","iopub.status.idle":"2025-04-04T13:53:19.558168Z","shell.execute_reply":"2025-04-04T13:53:19.557239Z"},"papermill":{"duration":0.070388,"end_time":"2025-04-04T13:53:19.559861","exception":false,"start_time":"2025-04-04T13:53:19.489473","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%python --bg --proc infer_proc2\nfrom common_stuff import *\nstart_inference(gpu=2)","metadata":{"execution":{"iopub.execute_input":"2025-04-04T13:53:19.601727Z","iopub.status.busy":"2025-04-04T13:53:19.601279Z","iopub.status.idle":"2025-04-04T13:53:19.619819Z","shell.execute_reply":"2025-04-04T13:53:19.618106Z"},"papermill":{"duration":0.0484,"end_time":"2025-04-04T13:53:19.626974","exception":false,"start_time":"2025-04-04T13:53:19.578574","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%python --bg --proc infer_proc3\nfrom common_stuff import *\nstart_inference(gpu=3)","metadata":{"execution":{"iopub.execute_input":"2025-04-04T13:53:19.731884Z","iopub.status.busy":"2025-04-04T13:53:19.731335Z","iopub.status.idle":"2025-04-04T13:53:19.749241Z","shell.execute_reply":"2025-04-04T13:53:19.744967Z"},"papermill":{"duration":0.080755,"end_time":"2025-04-04T13:53:19.751783","exception":false,"start_time":"2025-04-04T13:53:19.671028","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"proc_exit_codes = await wait_for_subprocesses(\n    train_proc0, train_proc1, train_proc2, train_proc3,\n    infer_proc0, infer_proc1, infer_proc2, infer_proc3,\n    print_output=True or arc_test_set.is_fake\n)\nprint(f'*** Subprocesses exit codes: {proc_exit_codes}')\nassert all(x==0 for x in proc_exit_codes)","metadata":{"execution":{"iopub.execute_input":"2025-04-04T13:53:19.830117Z","iopub.status.busy":"2025-04-04T13:53:19.829328Z","iopub.status.idle":"2025-04-04T14:08:44.63478Z","shell.execute_reply":"2025-04-04T14:08:44.634113Z"},"papermill":{"duration":924.842711,"end_time":"2025-04-04T14:08:44.63648","exception":false,"start_time":"2025-04-04T13:53:19.793769","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# write submission\nfrom common_stuff import *\nwith RemapCudaOOM():\n    model, formatter, dataset = None, MyFormatter(), None\n    decoder = Decoder(formatter, arc_test_set.split_multi_replies(), n_guesses=2, frac_score=True).from_store(infer_params['store'])\n    if use_aug_score or arc_test_set.is_fake: decoder.calc_augmented_scores(model=model, store=score_temp_storage, **aug_score_params)\n    submission = arc_test_set.get_submission(decoder.run_selection_algo(submission_select_algo))\n    with open('submission.json', 'w') as f: json.dump(submission, f)\n    if arc_test_set.is_fake:\n        decoder.benchmark_selection_algos(selection_algorithms)\n        with open('submission.json') as f: reload_submission = json.load(f)\n        print('*** Reload score:', arc_test_set.validate_submission(reload_submission))","metadata":{"execution":{"iopub.execute_input":"2025-04-04T14:08:44.811356Z","iopub.status.busy":"2025-04-04T14:08:44.810792Z","iopub.status.idle":"2025-04-04T14:08:45.692749Z","shell.execute_reply":"2025-04-04T14:08:45.692117Z"},"papermill":{"duration":0.969844,"end_time":"2025-04-04T14:08:45.694257","exception":false,"start_time":"2025-04-04T14:08:44.724413","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualization for inference results from submission.json\nif arc_test_set.is_fake:\n    from common_stuff import *\n    import matplotlib.pyplot as plt\n    from matplotlib import colors\n    import json\n    import os\n    import numpy as np\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"VISUALIZING RESULTS FROM SUBMISSION.JSON\")\n    print(\"=\"*80)\n    \n    # Check if submission file exists\n    submission_path = 'submission.json'\n    if not os.path.exists(submission_path):\n        print(f\"Submission file not found at {submission_path}\")\n    else:\n        print(f\"Found submission file: {submission_path}\")\n        \n        # Load submission data\n        with open(submission_path, 'r') as f:\n            submission_data = json.load(f)\n        \n        print(f\"Loaded submission with {len(submission_data)} tasks\")\n        \n        # ARC color map\n        cmap = colors.ListedColormap(\n            ['#000000', '#0074D9', '#FF4136', '#2ECC40', '#FFDC00',\n             '#AAAAAA', '#F012BE', '#FF851B', '#7FDBFF', '#870C25'])\n        norm = colors.Normalize(vmin=0, vmax=9)\n        \n        # Function to check if prediction is non-trivial (not just zeros)\n        def is_non_trivial_prediction(pred_array):\n            # Check if the prediction contains any non-zero values\n            return np.any(np.array(pred_array) > 0)\n        \n        # Function to visualize a single task result\n        def visualize_submission_result(task_id, task_data, submission_output, test_idx):\n            # Skip visualization if both predictions are just zeros\n            pred_1 = np.array(submission_output['attempt_1'])\n            pred_2 = np.array(submission_output['attempt_2'])\n            \n            if not is_non_trivial_prediction(pred_1) and not is_non_trivial_prediction(pred_2):\n                print(f\"  Skipping visualization for Task {task_id} - Test #{test_idx+1} (all predictions are zeros)\")\n                return False\n            \n            # Create visualization\n            fig = plt.figure(figsize=(15, 8))\n            grid_spec = plt.GridSpec(2, 3, width_ratios=[1, 1, 1])\n            \n            # Training examples (first one only for simplicity)\n            if task_data['train']:\n                # Train Input\n                ax1 = fig.add_subplot(grid_spec[0, 0])\n                ax1.imshow(task_data['train'][0]['input'], cmap=cmap, norm=norm)\n                ax1.grid(True, which='both', color='lightgrey', linewidth=0.5)\n                ax1.set_title(\"Training Input\")\n                ax1.set_xticks([])\n                ax1.set_yticks([])\n                \n                # Train Output\n                ax2 = fig.add_subplot(grid_spec[1, 0])\n                ax2.imshow(task_data['train'][0]['output'], cmap=cmap, norm=norm)\n                ax2.grid(True, which='both', color='lightgrey', linewidth=0.5)\n                ax2.set_title(\"Training Output\")\n                ax2.set_xticks([])\n                ax2.set_yticks([])\n            \n            # Test Input\n            if test_idx < len(task_data['test']):\n                ax3 = fig.add_subplot(grid_spec[0, 1])\n                ax3.imshow(task_data['test'][test_idx]['input'], cmap=cmap, norm=norm)\n                ax3.grid(True, which='both', color='lightgrey', linewidth=0.5)\n                ax3.set_title(f\"Test Input (Test #{test_idx+1})\")\n                ax3.set_xticks([])\n                ax3.set_yticks([])\n                \n                # Ground Truth (if available)\n                if 'output' in task_data['test'][test_idx]:\n                    ax4 = fig.add_subplot(grid_spec[1, 1])\n                    ax4.imshow(task_data['test'][test_idx]['output'], cmap=cmap, norm=norm)\n                    ax4.grid(True, which='both', color='lightgrey', linewidth=0.5)\n                    ax4.set_title(\"Ground Truth\")\n                    ax4.set_xticks([])\n                    ax4.set_yticks([])\n            \n            # Model Predictions\n            # Attempt 1\n            ax5 = fig.add_subplot(grid_spec[0, 2])\n            ax5.imshow(pred_1, cmap=cmap, norm=norm)\n            ax5.grid(True, which='both', color='lightgrey', linewidth=0.5)\n            ax5.set_title(\"Model Prediction (Attempt 1)\")\n            ax5.set_xticks([])\n            ax5.set_yticks([])\n            \n            # Attempt 2\n            ax6 = fig.add_subplot(grid_spec[1, 2])\n            ax6.imshow(pred_2, cmap=cmap, norm=norm)\n            ax6.grid(True, which='both', color='lightgrey', linewidth=0.5)\n            ax6.set_title(\"Model Prediction (Attempt 2)\")\n            ax6.set_xticks([])\n            ax6.set_yticks([])\n            \n            plt.suptitle(f\"Task {task_id} - Test Example #{test_idx+1}\", fontsize=16)\n            plt.tight_layout()\n            plt.subplots_adjust(top=0.9)\n            plt.show()\n            \n            # Calculate accuracy if ground truth is available\n            if 'output' in task_data['test'][test_idx]:\n                ground_truth = np.array(task_data['test'][test_idx]['output'])\n                \n                # Check accuracy of both attempts\n                results = []\n                match_1 = np.array_equal(pred_1, ground_truth) if is_non_trivial_prediction(pred_1) else False\n                results.append(f\"Attempt 1: {'' if match_1 else ''}{' (zeros)' if not is_non_trivial_prediction(pred_1) else ''}\")\n                \n                match_2 = np.array_equal(pred_2, ground_truth) if is_non_trivial_prediction(pred_2) else False\n                results.append(f\"Attempt 2: {'' if match_2 else ''}{' (zeros)' if not is_non_trivial_prediction(pred_2) else ''}\")\n                \n                print(f\"  Results: {', '.join(results)}\")\n                \n                # Display task statistics\n                print(f\"  Shape - Ground Truth: {ground_truth.shape}, Prediction 1: {pred_1.shape}, Prediction 2: {pred_2.shape}\")\n                print(f\"  Values - Ground Truth unique values: {np.unique(ground_truth)}\")\n                print(f\"          Prediction 1 unique values: {np.unique(pred_1)}\")\n                print(f\"          Prediction 2 unique values: {np.unique(pred_2)}\")\n            print()\n            return True\n        \n        # Process ALL results from submission (no limit)\n        visualized_count = 0\n        skipped_count = 0\n        \n        # Get a list of tasks in the submission\n        task_ids = list(submission_data.keys())\n        \n        # Collect all task/test combinations\n        all_predictions = []\n        for task_id in task_ids:\n            if task_id in arc_test_set.queries:\n                task_data = arc_test_set.queries[task_id]\n                for test_idx, test_prediction in enumerate(submission_data[task_id]):\n                    # Check if we have ground truth available\n                    has_ground_truth = (task_id in arc_test_set.replies and \n                                        test_idx < len(arc_test_set.replies[task_id]))\n                    \n                    # Check if predictions are non-trivial\n                    pred_1 = np.array(test_prediction['attempt_1'])\n                    pred_2 = np.array(test_prediction['attempt_2'])\n                    has_non_zero_pred = is_non_trivial_prediction(pred_1) or is_non_trivial_prediction(pred_2)\n                    \n                    # Score based on correctness if ground truth is available\n                    score = 0\n                    if has_ground_truth and has_non_zero_pred:\n                        ground_truth = np.array(arc_test_set.replies[task_id][test_idx])\n                        \n                        match_1 = np.array_equal(pred_1, ground_truth) if is_non_trivial_prediction(pred_1) else False\n                        match_2 = np.array_equal(pred_2, ground_truth) if is_non_trivial_prediction(pred_2) else False\n                        score = match_1 + match_2\n                        \n                    all_predictions.append((task_id, test_idx, score, has_ground_truth, has_non_zero_pred))\n        \n        # Sort by whether they have ground truth first, then by score\n        all_predictions.sort(key=lambda x: (-int(x[3]), -x[2]))\n        \n        # Print summary before visualization\n        print(f\"\\nFound {len(all_predictions)} total predictions to visualize\")\n        \n        # Visualize all tasks\n        for task_id, test_idx, score, has_ground_truth, has_non_zero_pred in all_predictions:\n            # Get task data and predictions\n            task_data = arc_test_set.queries[task_id]\n            submission_output = submission_data[task_id][test_idx]\n            \n            # Visualize this task\n            score_info = f\" (Score: {score}/2)\" if has_ground_truth and has_non_zero_pred else \" (no ground truth)\" if not has_ground_truth else \" (all zeros - no score)\"\n            print(f\"\\nTask: {task_id} - Test #{test_idx+1}{score_info}\")\n            \n            # Only increment visualized_count if actually visualized\n            if visualize_submission_result(task_id, task_data, submission_output, test_idx):\n                visualized_count += 1\n            else:\n                skipped_count += 1\n        \n        print(f\"\\nVisualized {visualized_count} inference results (skipped {skipped_count} with all-zero predictions)\")\n        \n        # Calculate overall accuracy statistics\n        if arc_test_set.is_fake:\n            total_tests = 0\n            total_scored_tests = 0\n            correct_attempt1 = 0\n            correct_attempt2 = 0\n            correct_any = 0\n            zero_predictions = 0\n            \n            for task_id, test_predictions in submission_data.items():\n                if task_id in arc_test_set.replies:\n                    for test_idx, test_prediction in enumerate(test_predictions):\n                        if test_idx < len(arc_test_set.replies[task_id]):\n                            total_tests += 1\n                            \n                            ground_truth = np.array(arc_test_set.replies[task_id][test_idx])\n                            pred_1 = np.array(test_prediction['attempt_1'])\n                            pred_2 = np.array(test_prediction['attempt_2'])\n                            \n                            # Check if both predictions are all zeros\n                            if not is_non_trivial_prediction(pred_1) and not is_non_trivial_prediction(pred_2):\n                                zero_predictions += 1\n                                continue\n                            \n                            # Only count tests with at least one non-zero prediction\n                            total_scored_tests += 1\n                            \n                            match_1 = np.array_equal(pred_1, ground_truth) if is_non_trivial_prediction(pred_1) else False\n                            match_2 = np.array_equal(pred_2, ground_truth) if is_non_trivial_prediction(pred_2) else False\n                            \n                            if match_1: correct_attempt1 += 1\n                            if match_2: correct_attempt2 += 1\n                            if match_1 or match_2: correct_any += 1\n            \n            if total_tests > 0:\n                print(\"\\n\" + \"=\"*80)\n                print(\"OVERALL ACCURACY STATISTICS\")\n                print(\"=\"*80)\n                print(f\"Total test examples: {total_tests}\")\n                print(f\"Test examples with zero predictions (excluded from accuracy): {zero_predictions}\")\n                print(f\"Test examples included in accuracy calculation: {total_scored_tests}\")\n                \n                if total_scored_tests > 0:\n                    print(f\"Correct on attempt 1: {correct_attempt1}/{total_scored_tests} ({correct_attempt1/total_scored_tests:.2%})\")\n                    print(f\"Correct on attempt 2: {correct_attempt2}/{total_scored_tests} ({correct_attempt2/total_scored_tests:.2%})\")\n                    print(f\"Correct on either attempt: {correct_any}/{total_scored_tests} ({correct_any/total_scored_tests:.2%})\")\n                else:\n                    print(\"No non-zero predictions to calculate accuracy\")\n                    \n                print(f\"Overall completion rate: {total_scored_tests/total_tests:.2%} of tests have non-zero predictions\")\n                print(\"=\"*80)\nelse:\n    print(\"Skipping inference visualization - not in fake test mode\")","metadata":{"execution":{"iopub.execute_input":"2025-04-04T14:08:45.873159Z","iopub.status.busy":"2025-04-04T14:08:45.872874Z","iopub.status.idle":"2025-04-04T14:08:52.871766Z","shell.execute_reply":"2025-04-04T14:08:52.871151Z"},"papermill":{"duration":7.089823,"end_time":"2025-04-04T14:08:52.873753","exception":false,"start_time":"2025-04-04T14:08:45.78393","status":"completed"},"tags":[]},"outputs":[],"execution_count":null}]}